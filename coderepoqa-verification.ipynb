{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13705718,"sourceType":"datasetVersion","datasetId":8718737}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages for multi-repository evaluation\n!pip install transformers accelerate evaluate rouge-score nltk datasets torch pandas numpy tqdm -q\nprint(\"‚úÖ All packages installed for multi-repository evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:08:01.357507Z","iopub.execute_input":"2025-11-14T19:08:01.358193Z","iopub.status.idle":"2025-11-14T19:08:05.392157Z","shell.execute_reply.started":"2025-11-14T19:08:01.358167Z","shell.execute_reply":"2025-11-14T19:08:05.391210Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ All packages installed for multi-repository evaluation\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Import all necessary libraries for multi-repository evaluation\nimport json\nimport glob\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Check GPU availability\nprint(\"üîß Environment Setup Complete!\")\nprint(f\"Python version: {torch.__version__}\")\nprint(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n# Set up paths for CodeRepoQA dataset\nCODEREPOQA_BASE_PATH = \"/kaggle/input/coderepoqa\"\nprint(f\"üìÇ CodeRepoQA base path: {CODEREPOQA_BASE_PATH}\")\n\n# Initialize variables that will be set by repository detection\nrepositories = {}\nall_json_files = []\n\nprint(\"‚úÖ Libraries imported and environment configured for multi-repository evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:08:09.230775Z","iopub.execute_input":"2025-11-14T19:08:09.231074Z","iopub.status.idle":"2025-11-14T19:08:09.239523Z","shell.execute_reply.started":"2025-11-14T19:08:09.231046Z","shell.execute_reply":"2025-11-14T19:08:09.238665Z"}},"outputs":[{"name":"stdout","text":"üîß Environment Setup Complete!\nPython version: 2.6.0+cu124\nPyTorch CUDA available: True\nCUDA device: Tesla T4\nGPU memory: 15.8 GB\nüìÇ CodeRepoQA base path: /kaggle/input/coderepoqa\n‚úÖ Libraries imported and environment configured for multi-repository evaluation\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"def create_test_samples_full_dataset(json_files, max_files=None, min_comment_length=10):\n    \"\"\"\n    Process ALL available data to create test samples\n    Following paper methodology: use historical dialogue as input, \n    last maintainer response as ground truth\n    \"\"\"\n    samples = []\n    skipped_stats = {\n        'no_comments': 0,\n        'no_maintainer_responses': 0,\n        'short_comments': 0,\n        'processing_errors': 0\n    }\n    issues_processed = 0\n    \n    files_to_process = json_files[:max_files] if max_files else json_files\n    \n    print(f\"üîÑ Processing {len(files_to_process):,} issues to create test samples...\")\n    \n    for file_path in tqdm(files_to_process, desc=\"Creating test samples\"):\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n            \n            issues_processed += 1\n            \n            # Build conversation chronologically\n            conversation = []\n            \n            # 1. Initial issue (always include)\n            issue_title = data.get('title', '').strip()\n            issue_body = data.get('body', '').strip()\n            initial_content = f\"Title: {issue_title}\\n\\nBody: {issue_body}\"\n            \n            conversation.append({\n                'speaker': 'user',\n                'content': initial_content,\n                'role': data.get('author_association', 'NONE'),\n                'timestamp': data.get('created_at', '')\n            })\n            \n            # 2. Get and process comments\n            comments = data.get('comments_details', [])\n            \n            if not comments:\n                skipped_stats['no_comments'] += 1\n                continue\n            \n            # 3. Add all valid comments to conversation\n            for comment in comments:\n                body = comment.get('body', '').strip()\n                role = comment.get('author_association', 'NONE')\n                \n                # Skip very short or empty comments\n                if not body or len(body) < min_comment_length:\n                    skipped_stats['short_comments'] += 1\n                    continue\n                \n                conversation.append({\n                    'speaker': 'maintainer' if role in ['MEMBER', 'CONTRIBUTOR', 'OWNER', 'COLLABORATOR'] \n                              else 'user',\n                    'content': body,\n                    'role': role,\n                    'timestamp': comment.get('created_at', '')\n                })\n            \n            # 4. Find all maintainer response positions\n            maintainer_indices = [\n                i for i, turn in enumerate(conversation) \n                if turn['speaker'] == 'maintainer'\n            ]\n            \n            if not maintainer_indices:\n                skipped_stats['no_maintainer_responses'] += 1\n                continue\n            \n            # 5. Create one test sample per maintainer response\n            for maintainer_idx in maintainer_indices:\n                context_turns = conversation[:maintainer_idx]\n                ground_truth = conversation[maintainer_idx]['content']\n                \n                # Skip if context is too short (need at least initial issue)\n                if len(context_turns) < 1:\n                    continue\n                \n                samples.append({\n                    'issue_number': data.get('number', 'unknown'),\n                    'context': context_turns,\n                    'ground_truth': ground_truth,\n                    'turn_number': maintainer_indices.index(maintainer_idx) + 1,\n                    'total_maintainer_turns': len(maintainer_indices),\n                    'total_conversation_turns': len(conversation),\n                    'maintainer_role': conversation[maintainer_idx]['role'],\n                    'context_char_length': sum(len(turn['content']) for turn in context_turns),\n                    'ground_truth_length': len(ground_truth),\n                    'file_path': file_path\n                })\n                \n        except Exception as e:\n            skipped_stats['processing_errors'] += 1\n            if skipped_stats['processing_errors'] <= 5:  # Show first 5 errors\n                print(f\"\\n‚ö†Ô∏è  Error processing {Path(file_path).name}: {str(e)[:100]}...\")\n            continue\n    \n    # Print comprehensive processing summary\n    print(f\"\\n=== SAMPLE CREATION SUMMARY ===\")\n    print(f\"üìÅ Issues processed: {issues_processed:,}\")\n    print(f\"‚úÖ Test samples created: {len(samples):,}\")\n    print(f\"üìä Average samples per valid issue: {len(samples)/(issues_processed-sum(skipped_stats.values())):.2f}\")\n    \n    print(f\"\\nüö´ Skipped Issues Breakdown:\")\n    print(f\"   No comments: {skipped_stats['no_comments']:,}\")\n    print(f\"   No maintainer responses: {skipped_stats['no_maintainer_responses']:,}\")\n    print(f\"   Short comments filtered: {skipped_stats['short_comments']:,}\")\n    print(f\"   Processing errors: {skipped_stats['processing_errors']:,}\")\n    print(f\"   Total skipped: {sum(skipped_stats.values()):,}\")\n    \n    if len(samples) > 0:\n        # Additional statistics\n        context_lengths = [s['context_char_length'] for s in samples]\n        gt_lengths = [s['ground_truth_length'] for s in samples]\n        \n        print(f\"\\nüìè Sample Characteristics:\")\n        print(f\"   Context length - Mean: {np.mean(context_lengths):.0f}, Median: {np.median(context_lengths):.0f}\")\n        print(f\"   Ground truth length - Mean: {np.mean(gt_lengths):.0f}, Median: {np.median(gt_lengths):.0f}\")\n        print(f\"   Samples per issue range: 1-{max([s['total_maintainer_turns'] for s in samples])}\")\n    \n    return samples, skipped_stats\n\nprint(\"‚úÖ Sample creation function loaded successfully!\")\nprint(\"üìù This function will be used by the multi-repository evaluation\")\nprint(\"üí° For multi-repository evaluation, samples are loaded per repository dynamically\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:08:44.868247Z","iopub.execute_input":"2025-11-14T19:08:44.869103Z","iopub.status.idle":"2025-11-14T19:08:44.885056Z","shell.execute_reply.started":"2025-11-14T19:08:44.869071Z","shell.execute_reply":"2025-11-14T19:08:44.884079Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Sample creation function loaded successfully!\nüìù This function will be used by the multi-repository evaluation\nüí° For multi-repository evaluation, samples are loaded per repository dynamically\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# Dataset Processing Status Check\nprint(\"üìä Multi-Repository Dataset Processing Status\")\nprint(\"=\" * 50)\n\n# Check if repositories have been detected\nif repositories:\n    print(f\"‚úÖ Repositories detected: {len(repositories)}\")\n    total_files = sum(repo_info['file_count'] for repo_info in repositories.values())\n    print(f\"‚úÖ Total JSON files found: {total_files:,}\")\n    \n    # Show repository summary\n    print(f\"\\nüìÅ Repository Summary:\")\n    for repo_name, repo_info in list(repositories.items())[:5]:  # Show first 5\n        print(f\"   ‚Ä¢ {repo_name}: {repo_info['file_count']} files\")\n    \n    if len(repositories) > 5:\n        print(f\"   ... and {len(repositories) - 5} more repositories\")\n        \n    print(f\"\\nüöÄ Ready for multi-repository evaluation!\")\n    print(f\"üí° Use the multi-repository evaluation functions to process all repositories\")\n    print(f\"   Examples:\")\n    print(f\"   ‚Ä¢ run_complete_multi_repository_evaluation() for full evaluation\")\n    print(f\"   ‚Ä¢ Individual repository processing using detected repository info\")\n    \nelse:\n    print(\"‚ö†Ô∏è  No repositories detected yet.\")\n    print(\"   Please run the repository detection cell first.\")\n    print(\"   The multi-repository evaluation will load samples dynamically per repository.\")\n\nprint(f\"\\nüí° Note: This notebook uses dynamic sample loading per repository\")\nprint(f\"   instead of loading all samples at once, which is more memory efficient\")\nprint(f\"   for multi-repository evaluation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:08:51.216324Z","iopub.execute_input":"2025-11-14T19:08:51.216635Z","iopub.status.idle":"2025-11-14T19:08:51.223745Z","shell.execute_reply.started":"2025-11-14T19:08:51.216617Z","shell.execute_reply":"2025-11-14T19:08:51.223028Z"}},"outputs":[{"name":"stdout","text":"üìä Multi-Repository Dataset Processing Status\n==================================================\n‚ö†Ô∏è  No repositories detected yet.\n   Please run the repository detection cell first.\n   The multi-repository evaluation will load samples dynamically per repository.\n\nüí° Note: This notebook uses dynamic sample loading per repository\n   instead of loading all samples at once, which is more memory efficient\n   for multi-repository evaluation.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# Import and setup evaluation metrics\nfrom evaluate import load\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Download required NLTK data\ntry:\n    nltk.download('punkt', quiet=True)\n    print(\"‚úÖ NLTK data downloaded\")\nexcept:\n    print(\"‚ö†Ô∏è NLTK download failed, but may already be available\")\n\n# Load ROUGE metric\nrouge = load('rouge')\nprint(\"‚úÖ ROUGE metric loaded\")\n\ndef calculate_all_metrics(prediction, ground_truth):\n    \"\"\"\n    Calculate all metrics from the CodeRepoQA paper:\n    - BLEU Score (with smoothing)\n    - ROUGE-L (Longest Common Subsequence)\n    - ROUGE-1 (Unigram overlap)\n    - Edit Similarity (Normalized Levenshtein distance)\n    \"\"\"\n    \n    # Clean and validate inputs\n    if not prediction or not ground_truth:\n        return {'bleu': 0.0, 'rouge_l': 0.0, 'rouge_1': 0.0, 'edit_similarity': 0.0}\n    \n    prediction = str(prediction).strip()\n    ground_truth = str(ground_truth).strip()\n    \n    if not prediction or not ground_truth:\n        return {'bleu': 0.0, 'rouge_l': 0.0, 'rouge_1': 0.0, 'edit_similarity': 0.0}\n    \n    # 1. BLEU Score (with smoothing function like the paper)\n    try:\n        # Tokenize for BLEU calculation\n        prediction_tokens = prediction.split()\n        ground_truth_tokens = [ground_truth.split()]  # BLEU expects list of reference lists\n        \n        # Use smoothing function to handle edge cases\n        smoothie = SmoothingFunction().method4\n        bleu_score = sentence_bleu(ground_truth_tokens, prediction_tokens, smoothing_function=smoothie)\n    except:\n        bleu_score = 0.0\n    \n    # 2. ROUGE Scores (L and 1)\n    try:\n        rouge_scores = rouge.compute(predictions=[prediction], references=[ground_truth])\n        rouge_l_score = rouge_scores['rougeL']\n        rouge_1_score = rouge_scores['rouge1']\n    except:\n        rouge_l_score = 0.0\n        rouge_1_score = 0.0\n    \n    # 3. Edit Similarity (Normalized Levenshtein Distance)\n    try:\n        edit_similarity = calculate_edit_similarity(prediction, ground_truth)\n    except:\n        edit_similarity = 0.0\n    \n    return {\n        'bleu': float(bleu_score),\n        'rouge_l': float(rouge_l_score),\n        'rouge_1': float(rouge_1_score),\n        'edit_similarity': float(edit_similarity)\n    }\n\ndef calculate_edit_similarity(prediction, ground_truth):\n    \"\"\"\n    Calculate normalized edit similarity (1 - normalized Levenshtein distance)\n    Following the paper's implementation\n    \"\"\"\n    def levenshtein_distance(s1, s2):\n        \"\"\"Calculate Levenshtein distance between two strings\"\"\"\n        if len(s1) < len(s2):\n            return levenshtein_distance(s2, s1)\n        \n        if len(s2) == 0:\n            return len(s1)\n        \n        previous_row = list(range(len(s2) + 1))\n        for i, c1 in enumerate(s1):\n            current_row = [i + 1]\n            for j, c2 in enumerate(s2):\n                insertions = previous_row[j + 1] + 1\n                deletions = current_row[j] + 1\n                substitutions = previous_row[j] + (c1 != c2)\n                current_row.append(min(insertions, deletions, substitutions))\n            previous_row = current_row\n        \n        return previous_row[-1]\n    \n    # Calculate Levenshtein distance\n    distance = levenshtein_distance(prediction, ground_truth)\n    \n    # Normalize by maximum possible distance (length of longer string)\n    max_len = max(len(prediction), len(ground_truth))\n    if max_len == 0:\n        return 1.0  # Both strings are empty\n    \n    # Convert distance to similarity (1 - normalized distance)\n    normalized_distance = distance / max_len\n    similarity = 1.0 - normalized_distance\n    \n    return max(0.0, similarity)  # Ensure non-negative\n\ndef format_conversation_context(context_turns):\n    \"\"\"Format conversation context for model input\"\"\"\n    formatted_context = []\n    \n    for turn in context_turns:\n        speaker = turn['speaker']\n        content = turn['content']\n        role = turn.get('role', 'USER')\n        \n        # Format each turn clearly\n        if speaker == 'user':\n            formatted_context.append(f\"USER: {content}\")\n        else:\n            formatted_context.append(f\"MAINTAINER ({role}): {content}\")\n    \n    return \"\\n\\n\".join(formatted_context)\n\nprint(\"‚úÖ All evaluation metrics implemented successfully!\")\nprint(\"üìä Available metrics: BLEU, ROUGE-L, ROUGE-1, Edit Similarity\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:08:55.620830Z","iopub.execute_input":"2025-11-14T19:08:55.621128Z","iopub.status.idle":"2025-11-14T19:08:56.264202Z","shell.execute_reply.started":"2025-11-14T19:08:55.621104Z","shell.execute_reply":"2025-11-14T19:08:56.263557Z"}},"outputs":[{"name":"stdout","text":"‚úÖ NLTK data downloaded\n‚úÖ ROUGE metric loaded\n‚úÖ All evaluation metrics implemented successfully!\nüìä Available metrics: BLEU, ROUGE-L, ROUGE-1, Edit Similarity\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# Model configurations - matching paper's evaluated models\nAVAILABLE_MODELS = {\n    \"deepseek-coder-1.3b\": {\n        \"name\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n        \"paper_name\": \"DeepSeek-Coder-1.3B\",\n        \"description\": \"Code-focused model, similar to paper's DSC-6.7B\"\n    },\n    \"codeqwen-7b\": {\n        \"name\": \"Qwen/CodeQwen1.5-7B-Chat\",\n        \"paper_name\": \"CodeQwen-7B\", \n        \"description\": \"Qwen's code model, similar to paper's CQ-7B\"\n    },\n    \"codellama-7b\": {\n        \"name\": \"codellama/CodeLlama-7b-Instruct-hf\",\n        \"paper_name\": \"CodeLlama-7B\",\n        \"description\": \"Meta's code-specialized model\"\n    },\n    \"mistral-7b\": {\n        \"name\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n        \"paper_name\": \"Mistral-7B\",\n        \"description\": \"General purpose instruction-tuned model\"\n    }\n}\n\ndef load_model_with_config(model_name, max_memory_gb=None):\n    \"\"\"\n    Load model and tokenizer with optimized configuration for evaluation\n    \"\"\"\n    print(f\"üîÑ Loading {model_name}...\")\n    start_time = time.time()\n    \n    try:\n        # Configure tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name, \n            trust_remote_code=True,\n            padding_side='left'\n        )\n        \n        # Set pad token if not exists\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        \n        # Configure model loading parameters\n        model_kwargs = {\n            \"trust_remote_code\": True,\n            \"low_cpu_mem_usage\": True,\n        }\n        \n        # Set appropriate dtype and device mapping\n        if torch.cuda.is_available():\n            model_kwargs.update({\n                \"torch_dtype\": torch.float16,\n                \"device_map\": \"auto\"\n            })\n            \n            # Handle memory constraints if specified\n            if max_memory_gb:\n                max_memory = {0: f\"{max_memory_gb}GB\"}\n                model_kwargs[\"max_memory\"] = max_memory\n        else:\n            model_kwargs[\"torch_dtype\"] = torch.float32\n        \n        # Load model\n        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n        \n        load_time = time.time() - start_time\n        \n        # Print model info\n        print(f\"‚úÖ Model loaded successfully in {load_time:.1f}s\")\n        print(f\"   Device: {next(model.parameters()).device}\")\n        print(f\"   dtype: {next(model.parameters()).dtype}\")\n        \n        if torch.cuda.is_available():\n            memory_allocated = torch.cuda.memory_allocated() / 1e9\n            print(f\"   GPU memory allocated: {memory_allocated:.1f} GB\")\n        \n        return model, tokenizer\n    \n    except Exception as e:\n        print(f\"‚ùå Error loading {model_name}: {e}\")\n        return None, None\n\ndef clear_model_memory(model):\n    \"\"\"Clear model from memory to free up GPU resources\"\"\"\n    if model is not None:\n        del model\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n# Display available models\nprint(\"ü§ñ Available Models for Evaluation:\")\nprint(\"=\" * 60)\nfor key, config in AVAILABLE_MODELS.items():\n    print(f\"üì¶ {key}:\")\n    print(f\"   HuggingFace: {config['name']}\")\n    print(f\"   Paper name: {config['paper_name']}\")\n    print(f\"   Description: {config['description']}\")\n    print()\n\nprint(\"üí° To evaluate a specific model, use:\")\nprint(\"   model, tokenizer = load_model_with_config(AVAILABLE_MODELS['deepseek-coder-6.7b']['name'])\")\nprint(\"\\n‚ö†Ô∏è  Note: Models require significant GPU memory. Load one at a time for evaluation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:09:00.397424Z","iopub.execute_input":"2025-11-14T19:09:00.398006Z","iopub.status.idle":"2025-11-14T19:09:00.408857Z","shell.execute_reply.started":"2025-11-14T19:09:00.397981Z","shell.execute_reply":"2025-11-14T19:09:00.407902Z"}},"outputs":[{"name":"stdout","text":"ü§ñ Available Models for Evaluation:\n============================================================\nüì¶ deepseek-coder-1.3b:\n   HuggingFace: deepseek-ai/deepseek-coder-6.7b-instruct\n   Paper name: DeepSeek-Coder-1.3B\n   Description: Code-focused model, similar to paper's DSC-6.7B\n\nüì¶ codeqwen-7b:\n   HuggingFace: Qwen/CodeQwen1.5-7B-Chat\n   Paper name: CodeQwen-7B\n   Description: Qwen's code model, similar to paper's CQ-7B\n\nüì¶ codellama-7b:\n   HuggingFace: codellama/CodeLlama-7b-Instruct-hf\n   Paper name: CodeLlama-7B\n   Description: Meta's code-specialized model\n\nüì¶ mistral-7b:\n   HuggingFace: mistralai/Mistral-7B-Instruct-v0.2\n   Paper name: Mistral-7B\n   Description: General purpose instruction-tuned model\n\nüí° To evaluate a specific model, use:\n   model, tokenizer = load_model_with_config(AVAILABLE_MODELS['deepseek-coder-6.7b']['name'])\n\n‚ö†Ô∏è  Note: Models require significant GPU memory. Load one at a time for evaluation.\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"def evaluate_model_comprehensive(model, tokenizer, samples, model_name, \n                                max_samples=None, generation_config=None, \n                                save_results=True):\n    \"\"\"\n    Comprehensive model evaluation on samples with detailed analysis\n    \"\"\"\n    \n    # Determine sample size\n    if max_samples and max_samples < len(samples):\n        # Use stratified sampling to get representative samples\n        sample_indices = np.linspace(0, len(samples)-1, max_samples, dtype=int)\n        eval_samples = [samples[i] for i in sample_indices]\n        print(f\"üìä Evaluating on {max_samples:,} stratified samples (from {len(samples):,} total)\")\n    else:\n        eval_samples = samples\n        print(f\"üìä Evaluating on all {len(eval_samples):,} samples\")\n    \n    results = []\n    failed_generations = 0\n    start_time = time.time()\n    \n    print(f\"üöÄ Starting evaluation of {model_name}...\")\n    \n    # Generate responses\n    responses = batch_generate_responses(\n        model, tokenizer, eval_samples, \n        batch_size=1, generation_config=generation_config\n    )\n    \n    # Calculate metrics for each response\n    print(\"üìè Calculating evaluation metrics...\")\n    for i, (sample, response) in enumerate(tqdm(zip(eval_samples, responses), \n                                                desc=\"Computing metrics\", \n                                                total=len(eval_samples))):\n        \n        if not response:  # Handle failed generations\n            failed_generations += 1\n            metrics = {'bleu': 0.0, 'rouge_l': 0.0, 'rouge_1': 0.0, 'edit_similarity': 0.0}\n        else:\n            metrics = calculate_all_metrics(response, sample['ground_truth'])\n        \n        # Store comprehensive results\n        result = {\n            'model_name': model_name,\n            'sample_idx': i,\n            'issue_number': sample['issue_number'],\n            'turn_number': sample['turn_number'],\n            'total_turns': sample['total_conversation_turns'],\n            'context_length': len(format_conversation_context(sample['context'])),\n            'context_turns': len(sample['context']),\n            'ground_truth_length': len(sample['ground_truth']),\n            'response_length': len(response),\n            'maintainer_role': sample['maintainer_role'],\n            'prediction': response,\n            'ground_truth': sample['ground_truth'],\n            **metrics\n        }\n        \n        results.append(result)\n    \n    # Create results DataFrame\n    results_df = pd.DataFrame(results)\n    \n    # Calculate summary statistics\n    eval_time = time.time() - start_time\n    avg_metrics = results_df[['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']].mean()\n    \n    # Print evaluation summary\n    print(f\"\\n{'='*60}\")\n    print(f\"üéØ EVALUATION COMPLETE: {model_name}\")\n    print(f\"{'='*60}\")\n    print(f\"‚è±Ô∏è  Total evaluation time: {eval_time/60:.1f} minutes\")\n    print(f\"üìä Samples evaluated: {len(results_df):,}\")\n    print(f\"‚ùå Failed generations: {failed_generations}\")\n    print(f\"‚úÖ Success rate: {(len(results_df)-failed_generations)/len(results_df)*100:.1f}%\")\n    \n    print(f\"\\nüìà Average Scores:\")\n    for metric, score in avg_metrics.items():\n        print(f\"   {metric.upper():15}: {score:.4f}\")\n    \n    overall_avg = avg_metrics.mean()\n    print(f\"   {'OVERALL AVG':15}: {overall_avg:.4f}\")\n    \n    # Save results if requested\n    if save_results:\n        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"evaluation_results_{model_name.replace('/', '_')}_{timestamp}.csv\"\n        results_df.to_csv(filename, index=False)\n        print(f\"üíæ Results saved to: {filename}\")\n    \n    return results_df, avg_metrics\n\ndef compare_with_paper_results(results_df, model_name):\n    \"\"\"Compare evaluation results with paper benchmarks\"\"\"\n    \n    # Paper results from Table 3 (CodeRepoQA paper)\n    paper_benchmarks = {\n        'DeepSeek-Coder-1.3B': {'BLEU': 0.1110, 'Rouge-L': 0.1350, 'Rouge-1': 0.2215, 'Edit Similarity': 0.1689},\n        'CodeQwen-7B': {'BLEU': 0.1188, 'Rouge-L': 0.1392, 'Rouge-1': 0.2264, 'Edit Similarity': 0.1803},\n        'CodeLlama-7B': {'BLEU': 0.1035, 'Rouge-L': 0.1298, 'Rouge-1': 0.2134, 'Edit Similarity': 0.1598},\n        'GPT-4': {'BLEU': 0.1179, 'Rouge-L': 0.1330, 'Rouge-1': 0.2315, 'Edit Similarity': 0.1715}\n    }\n    \n    # Calculate our results\n    our_results = {\n        'BLEU': results_df['bleu'].mean(),\n        'Rouge-L': results_df['rouge_l'].mean(),\n        'Rouge-1': results_df['rouge_1'].mean(),\n        'Edit Similarity': results_df['edit_similarity'].mean()\n    }\n    \n    print(f\"\\nüìä COMPARISON WITH PAPER RESULTS\")\n    print(f\"{'='*50}\")\n    \n    # Find closest paper model for comparison\n    paper_model = None\n    if 'deepseek' in model_name.lower():\n        paper_model = 'DeepSeek-Coder-1.3B'\n    elif 'qwen' in model_name.lower():\n        paper_model = 'CodeQwen-7B'\n    elif 'codellama' in model_name.lower():\n        paper_model = 'CodeLlama-7B'\n    else:\n        paper_model = 'GPT-4'  # Default comparison\n    \n    # Create comparison DataFrame\n    comparison_data = {\n        'Metric': list(our_results.keys()),\n        f'Our Results ({model_name})': list(our_results.values()),\n        f'Paper ({paper_model})': list(paper_benchmarks[paper_model].values())\n    }\n    \n    comparison_df = pd.DataFrame(comparison_data)\n    comparison_df['Difference'] = comparison_df[f'Our Results ({model_name})'] - comparison_df[f'Paper ({paper_model})']\n    comparison_df['% Difference'] = (comparison_df['Difference'] / comparison_df[f'Paper ({paper_model})']) * 100\n    \n    print(comparison_df.round(4))\n    \n    print(f\"\\nüìù Notes:\")\n    print(f\"   ‚Ä¢ Paper evaluated on 585,687 samples across multiple repositories\")\n    print(f\"   ‚Ä¢ Our evaluation: {len(results_df):,} samples from AutoGPT repository only\")\n    print(f\"   ‚Ä¢ Differences expected due to sample size and repository specificity\")\n    \n    return comparison_df\n\n# Evaluation configuration\nEVALUATION_CONFIG = {\n    'full_dataset': {\n        'max_samples': None,  # Use all samples\n        'generation_config': {'max_new_tokens': 512, 'temperature': 0.7}\n    },\n    'large_sample': {\n        'max_samples': 1000,  # 1K samples for faster evaluation\n        'generation_config': {'max_new_tokens': 512, 'temperature': 0.7}\n    },\n    'medium_sample': {\n        'max_samples': 500,   # 500 samples\n        'generation_config': {'max_new_tokens': 512, 'temperature': 0.7}\n    },\n    'small_sample': {\n        'max_samples': 100,   # 100 samples for quick testing\n        'generation_config': {'max_new_tokens': 256, 'temperature': 0.7}\n    }\n}\n\nprint(\"üîß Evaluation pipeline ready!\")\nprint(f\"üìã Available configurations: {list(EVALUATION_CONFIG.keys())}\")\nprint(\"\\nüí° Usage example:\")\nprint(\"   # Load model\")\nprint(\"   model, tokenizer = load_model_with_config('deepseek-ai/deepseek-coder-6.7b-instruct')\")\nprint(\"   # Run evaluation\")\nprint(\"   results_df, metrics = evaluate_model_comprehensive(\")\nprint(\"       model, tokenizer, all_test_samples, 'deepseek-coder-6.7b',\")\nprint(\"       **EVALUATION_CONFIG['medium_sample'])\")\nprint(\"   # Compare with paper\")\nprint(\"   comparison = compare_with_paper_results(results_df, 'deepseek-coder-6.7b')\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:09:07.478795Z","iopub.execute_input":"2025-11-14T19:09:07.479624Z","iopub.status.idle":"2025-11-14T19:09:07.497974Z","shell.execute_reply.started":"2025-11-14T19:09:07.479599Z","shell.execute_reply":"2025-11-14T19:09:07.497322Z"}},"outputs":[{"name":"stdout","text":"üîß Evaluation pipeline ready!\nüìã Available configurations: ['full_dataset', 'large_sample', 'medium_sample', 'small_sample']\n\nüí° Usage example:\n   # Load model\n   model, tokenizer = load_model_with_config('deepseek-ai/deepseek-coder-6.7b-instruct')\n   # Run evaluation\n   results_df, metrics = evaluate_model_comprehensive(\n       model, tokenizer, all_test_samples, 'deepseek-coder-6.7b',\n       **EVALUATION_CONFIG['medium_sample'])\n   # Compare with paper\n   comparison = compare_with_paper_results(results_df, 'deepseek-coder-6.7b')\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"def analyze_results_comprehensive(results_df, model_name):\n    \"\"\"Comprehensive analysis of evaluation results\"\"\"\n    \n    print(f\"üîç COMPREHENSIVE RESULTS ANALYSIS: {model_name}\")\n    print(f\"{'='*60}\")\n    \n    # Basic statistics\n    print(f\"üìä Dataset Statistics:\")\n    print(f\"   Total samples: {len(results_df):,}\")\n    print(f\"   Unique issues: {results_df['issue_number'].nunique():,}\")\n    print(f\"   Turn distribution: {results_df['turn_number'].value_counts().sort_index().to_dict()}\")\n    \n    # Performance by metrics\n    print(f\"\\nüìà Performance Metrics:\")\n    metrics = ['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']\n    for metric in metrics:\n        mean_score = results_df[metric].mean()\n        std_score = results_df[metric].std()\n        median_score = results_df[metric].median()\n        print(f\"   {metric.upper():15}: Mean={mean_score:.4f}, Std={std_score:.4f}, Median={median_score:.4f}\")\n    \n    # Performance by context length\n    print(f\"\\nüìè Performance by Context Length:\")\n    results_df['context_length_bin'] = pd.qcut(\n        results_df['context_length'], \n        q=5, \n        labels=['Very Short', 'Short', 'Medium', 'Long', 'Very Long']\n    )\n    \n    length_analysis = results_df.groupby('context_length_bin')[metrics].mean()\n    print(length_analysis.round(4))\n    \n    # Find best performing context length\n    length_avg = length_analysis.mean(axis=1)\n    best_length = length_avg.idxmax()\n    print(f\"\\n‚ú® Best performing context length: {best_length} (avg score: {length_avg[best_length]:.4f})\")\n    \n    # Performance by turn number\n    print(f\"\\nüîÑ Performance by Turn Number:\")\n    turn_analysis = results_df.groupby('turn_number')[metrics].mean()\n    print(turn_analysis.round(4))\n    \n    # Performance by maintainer role\n    print(f\"\\nüë• Performance by Maintainer Role:\")\n    role_analysis = results_df.groupby('maintainer_role')[metrics].mean()\n    print(role_analysis.round(4))\n    \n    # Identify best and worst predictions\n    results_df['avg_score'] = results_df[metrics].mean(axis=1)\n    best_idx = results_df['avg_score'].idxmax()\n    worst_idx = results_df['avg_score'].idxmin()\n    \n    print(f\"\\nüèÜ Best Prediction (Score: {results_df.loc[best_idx, 'avg_score']:.4f}):\")\n    print(f\"   Issue #{results_df.loc[best_idx, 'issue_number']}, Turn {results_df.loc[best_idx, 'turn_number']}\")\n    print(f\"   Prediction: {results_df.loc[best_idx, 'prediction'][:200]}...\")\n    print(f\"   Ground Truth: {results_df.loc[best_idx, 'ground_truth'][:200]}...\")\n    \n    print(f\"\\nüîª Worst Prediction (Score: {results_df.loc[worst_idx, 'avg_score']:.4f}):\")\n    print(f\"   Issue #{results_df.loc[worst_idx, 'issue_number']}, Turn {results_df.loc[worst_idx, 'turn_number']}\")\n    print(f\"   Prediction: {results_df.loc[worst_idx, 'prediction'][:200]}...\")\n    print(f\"   Ground Truth: {results_df.loc[worst_idx, 'ground_truth'][:200]}...\")\n    \n    return {\n        'length_analysis': length_analysis,\n        'turn_analysis': turn_analysis,\n        'role_analysis': role_analysis,\n        'best_sample': results_df.loc[best_idx],\n        'worst_sample': results_df.loc[worst_idx]\n    }\n\ndef create_performance_visualizations(results_df, model_name):\n    \"\"\"Create visualizations for performance analysis\"\"\"\n    \n    try:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n        \n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        fig.suptitle(f'Performance Analysis: {model_name}', fontsize=16)\n        \n        # 1. Metric distribution\n        metrics = ['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']\n        results_df[metrics].boxplot(ax=axes[0,0])\n        axes[0,0].set_title('Metric Distributions')\n        axes[0,0].set_ylabel('Score')\n        \n        # 2. Performance by context length\n        if 'context_length_bin' in results_df.columns:\n            length_means = results_df.groupby('context_length_bin')[metrics].mean()\n            length_means.plot(kind='bar', ax=axes[0,1])\n            axes[0,1].set_title('Performance by Context Length')\n            axes[0,1].set_ylabel('Average Score')\n            axes[0,1].tick_params(axis='x', rotation=45)\n        \n        # 3. Score correlation heatmap\n        correlation_matrix = results_df[metrics].corr()\n        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=axes[1,0])\n        axes[1,0].set_title('Metric Correlations')\n        \n        # 4. Performance by turn number\n        turn_means = results_df.groupby('turn_number')[metrics].mean()\n        turn_means.plot(ax=axes[1,1])\n        axes[1,1].set_title('Performance by Turn Number')\n        axes[1,1].set_xlabel('Turn Number')\n        axes[1,1].set_ylabel('Average Score')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        return fig\n    \n    except ImportError:\n        print(\"üìä Matplotlib/Seaborn not available for visualizations\")\n        return None\n\ndef generate_evaluation_report(results_df, model_name, analysis_results, comparison_df=None):\n    \"\"\"Generate a comprehensive evaluation report\"\"\"\n    \n    report = f\"\"\"\n# CodeRepoQA Evaluation Report\n## Model: {model_name}\n### Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n---\n\n## Executive Summary\n\n**Model Performance:**\n- Total samples evaluated: {len(results_df):,}\n- Average BLEU score: {results_df['bleu'].mean():.4f}\n- Average ROUGE-L score: {results_df['rouge_l'].mean():.4f}\n- Average ROUGE-1 score: {results_df['rouge_1'].mean():.4f}\n- Average Edit Similarity: {results_df['edit_similarity'].mean():.4f}\n- **Overall Average Score: {results_df[['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']].mean().mean():.4f}**\n\n## Key Findings\n\n### Performance by Context Length\n{analysis_results['length_analysis'].round(4).to_string()}\n\n**Best performing context length:** {analysis_results['length_analysis'].mean(axis=1).idxmax()}\n\n### Performance by Turn Number\n{analysis_results['turn_analysis'].round(4).to_string()}\n\n### Performance by Maintainer Role\n{analysis_results['role_analysis'].round(4).to_string()}\n\n## Sample Analysis\n\n### Best Performing Sample\n- **Issue:** #{analysis_results['best_sample']['issue_number']}\n- **Turn:** {analysis_results['best_sample']['turn_number']}\n- **Score:** {analysis_results['best_sample'][['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']].mean():.4f}\n- **Context Length:** {analysis_results['best_sample']['context_length']} chars\n\n### Worst Performing Sample\n- **Issue:** #{analysis_results['worst_sample']['issue_number']}\n- **Turn:** {analysis_results['worst_sample']['turn_number']}\n- **Score:** {analysis_results['worst_sample'][['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']].mean():.4f}\n- **Context Length:** {analysis_results['worst_sample']['context_length']} chars\n\n---\n\n*Report generated by CodeRepoQA Evaluation Pipeline*\n\"\"\"\n    \n    # Save report\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n    report_filename = f\"evaluation_report_{model_name.replace('/', '_')}_{timestamp}.md\"\n    \n    with open(report_filename, 'w', encoding='utf-8') as f:\n        f.write(report)\n    \n    print(f\"üìÑ Evaluation report saved to: {report_filename}\")\n    return report_filename\n\nprint(\"üìä Results analysis functions ready!\")\nprint(\"üí° Usage:\")\nprint(\"   analysis = analyze_results_comprehensive(results_df, model_name)\")\nprint(\"   fig = create_performance_visualizations(results_df, model_name)\")\nprint(\"   report = generate_evaluation_report(results_df, model_name, analysis)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T18:45:58.816244Z","iopub.execute_input":"2025-11-14T18:45:58.816967Z","iopub.status.idle":"2025-11-14T18:45:58.835396Z","shell.execute_reply.started":"2025-11-14T18:45:58.816935Z","shell.execute_reply":"2025-11-14T18:45:58.834552Z"}},"outputs":[{"name":"stdout","text":"üìä Results analysis functions ready!\nüí° Usage:\n   analysis = analyze_results_comprehensive(results_df, model_name)\n   fig = create_performance_visualizations(results_df, model_name)\n   report = generate_evaluation_report(results_df, model_name, analysis)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"def analyze_context_length_performance(results_df, model_name):\n    \"\"\"\n    Detailed analysis of performance by context length\n    Validates paper's claim: \"Medium-length contexts are more conducive to LLMs' performance\"\n    \"\"\"\n    \n    print(f\"üìè CONTEXT LENGTH ANALYSIS: {model_name}\")\n    print(f\"{'='*60}\")\n    \n    # Create detailed length bins\n    context_lengths = results_df['context_length']\n    \n    # Create multiple binning strategies\n    binning_strategies = {\n        'quintiles': pd.qcut(context_lengths, q=5, labels=['Very Short', 'Short', 'Medium', 'Long', 'Very Long']),\n        'fixed_ranges': pd.cut(context_lengths, \n                              bins=[0, 500, 1500, 3000, 5000, float('inf')],\n                              labels=['<500', '500-1500', '1500-3000', '3000-5000', '5000+']),\n        'paper_inspired': pd.cut(context_lengths,\n                               bins=[0, 1000, 2500, 4000, float('inf')],\n                               labels=['Short', 'Medium-Short', 'Medium-Long', 'Long'])\n    }\n    \n    metrics = ['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']\n    \n    for strategy_name, bins in binning_strategies.items():\n        print(f\"\\nüìä {strategy_name.upper()} Analysis:\")\n        \n        # Add bins to dataframe\n        temp_df = results_df.copy()\n        temp_df['length_bin'] = bins\n        \n        # Calculate performance by bin\n        bin_analysis = temp_df.groupby('length_bin')[metrics].agg(['mean', 'std', 'count'])\n        \n        # Show results\n        print(f\"\\nPerformance by {strategy_name}:\")\n        for metric in metrics:\n            print(f\"\\n{metric.upper()}:\")\n            for bin_name in bin_analysis.index:\n                mean_score = bin_analysis.loc[bin_name, (metric, 'mean')]\n                std_score = bin_analysis.loc[bin_name, (metric, 'std')]\n                count = bin_analysis.loc[bin_name, (metric, 'count')]\n                print(f\"  {bin_name:12}: {mean_score:.4f} ¬± {std_score:.4f} (n={count})\")\n        \n        # Find best performing bin for this strategy\n        overall_performance = temp_df.groupby('length_bin')[metrics].mean()\n        avg_performance = overall_performance.mean(axis=1)\n        best_bin = avg_performance.idxmax()\n        best_score = avg_performance[best_bin]\n        \n        print(f\"\\n‚ú® Best performing bin ({strategy_name}): {best_bin} (avg: {best_score:.4f})\")\n        \n        # Statistical significance testing (if scipy available)\n        try:\n            from scipy import stats\n            \n            # Compare best bin with others\n            best_bin_data = temp_df[temp_df['length_bin'] == best_bin][metrics].mean(axis=1)\n            other_bins_data = temp_df[temp_df['length_bin'] != best_bin][metrics].mean(axis=1)\n            \n            t_stat, p_value = stats.ttest_ind(best_bin_data, other_bins_data)\n            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n            \n            print(f\"üìà Statistical significance: t={t_stat:.3f}, p={p_value:.4f} {significance}\")\n            \n        except ImportError:\n            print(\"üìä Statistical testing requires scipy (not available)\")\n    \n    # Paper claim verification\n    print(f\"\\nüîç PAPER CLAIM VERIFICATION\")\n    print(f\"{'='*40}\")\n    print(f\"Paper claim: 'Medium-length contexts are more conducive to LLMs' performance'\")\n    \n    # Use paper-inspired binning for verification\n    temp_df = results_df.copy()\n    temp_df['length_bin'] = binning_strategies['paper_inspired']\n    avg_performance = temp_df.groupby('length_bin')[metrics].mean().mean(axis=1)\n    \n    print(f\"\\nOur findings:\")\n    for bin_name, score in avg_performance.items():\n        marker = \"üëë\" if score == avg_performance.max() else \"  \"\n        print(f\"{marker} {bin_name:12}: {score:.4f}\")\n    \n    best_category = avg_performance.idxmax()\n    is_medium = 'medium' in best_category.lower()\n    \n    print(f\"\\nüéØ Conclusion: \", end=\"\")\n    if is_medium:\n        print(f\"‚úÖ CONFIRMED - '{best_category}' contexts perform best\")\n    else:\n        print(f\"‚ùì PARTIALLY CONFIRMED - '{best_category}' contexts perform best (not medium)\")\n    \n    return {\n        'binning_strategies': binning_strategies,\n        'best_performing_bins': {name: temp_df.groupby('length_bin')[metrics].mean().mean(axis=1).idxmax() \n                               for name, bins in binning_strategies.items()},\n        'paper_claim_verified': is_medium\n    }\n\ndef detailed_context_analysis(results_df):\n    \"\"\"Additional detailed context analysis\"\"\"\n    \n    print(f\"\\nüîé DETAILED CONTEXT CHARACTERISTICS\")\n    print(f\"{'='*50}\")\n    \n    # Context length statistics\n    context_stats = results_df['context_length'].describe()\n    print(f\"üìä Context Length Statistics:\")\n    for stat, value in context_stats.items():\n        print(f\"   {stat:8}: {value:8.1f}\")\n    \n    # Correlation between context length and performance\n    metrics = ['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']\n    \n    print(f\"\\nüîó Correlation between Context Length and Performance:\")\n    for metric in metrics:\n        correlation = results_df['context_length'].corr(results_df[metric])\n        direction = \"positive\" if correlation > 0 else \"negative\"\n        strength = \"strong\" if abs(correlation) > 0.5 else \"moderate\" if abs(correlation) > 0.3 else \"weak\"\n        print(f\"   {metric.upper():15}: {correlation:6.3f} ({strength} {direction})\")\n    \n    # Context turns vs performance\n    print(f\"\\nüîÑ Context Turns vs Performance:\")\n    turn_performance = results_df.groupby('context_turns')[metrics].mean()\n    print(turn_performance.round(4))\n    \n    return {\n        'context_stats': context_stats,\n        'length_correlations': {metric: results_df['context_length'].corr(results_df[metric]) for metric in metrics},\n        'turn_performance': turn_performance\n    }\n\nprint(\"üìè Context length analysis functions ready!\")\nprint(\"üí° Usage:\")\nprint(\"   context_analysis = analyze_context_length_performance(results_df, model_name)\")\nprint(\"   detailed_analysis = detailed_context_analysis(results_df)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:09:27.639352Z","iopub.execute_input":"2025-11-14T19:09:27.639994Z","iopub.status.idle":"2025-11-14T19:09:27.656145Z","shell.execute_reply.started":"2025-11-14T19:09:27.639966Z","shell.execute_reply":"2025-11-14T19:09:27.655203Z"}},"outputs":[{"name":"stdout","text":"üìè Context length analysis functions ready!\nüí° Usage:\n   context_analysis = analyze_context_length_performance(results_df, model_name)\n   detailed_analysis = detailed_context_analysis(results_df)\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# COMPREHENSIVE FULL DATASET EVALUATION\n# This section runs evaluation on ALL available data\n\ndef run_full_evaluation_suite():\n    \"\"\"\n    Run comprehensive evaluation on the complete dataset\n    \"\"\"\n    \n    if 'all_test_samples' not in locals() or len(all_test_samples) == 0:\n        print(\"‚ùå No test samples available! Run the data processing section first.\")\n        return\n    \n    print(\"üöÄ STARTING COMPREHENSIVE FULL DATASET EVALUATION\")\n    print(\"=\" * 60)\n    print(f\"üìä Total samples to evaluate: {len(all_test_samples):,}\")\n    print(f\"üìÅ Source files: {len(json_files):,} JSON files\")\n    print(f\"‚è±Ô∏è  Estimated time: {len(all_test_samples) * 0.5 / 60:.1f} minutes per model\")\n    \n    # Results storage\n    all_model_results = {}\n    evaluation_summary = []\n    \n    # Models to evaluate (modify based on available resources)\n    models_to_evaluate = [\n        \"deepseek-coder-6.7b\",  # Primary model from paper\n        # \"codeqwen-7b\",        # Uncomment if you have sufficient GPU memory\n        # \"codellama-7b\",       # Uncomment if you have sufficient GPU memory\n        # \"mistral-7b\"          # Uncomment if you have sufficient GPU memory\n    ]\n    \n    print(f\"ü§ñ Models to evaluate: {models_to_evaluate}\")\n    print(f\"\\n‚ö†Ô∏è  Note: Evaluating one model at a time to manage memory\")\n    \n    for model_key in models_to_evaluate:\n        print(f\"\\n{'='*60}\")\n        print(f\"üîÑ EVALUATING MODEL: {model_key}\")\n        print(f\"{'='*60}\")\n        \n        model_config = AVAILABLE_MODELS[model_key]\n        model_name = model_config[\"name\"]\n        \n        try:\n            # Load model\n            print(f\"üì• Loading {model_name}...\")\n            model, tokenizer = load_model_with_config(model_name)\n            \n            if model is None:\n                print(f\"‚ùå Failed to load {model_key}, skipping...\")\n                continue\n            \n            # Run evaluation with different sample sizes\n            evaluation_configs = {\n                'full_dataset': {'max_samples': None, 'description': 'Complete dataset'},\n                # 'large_sample': {'max_samples': 2000, 'description': '2K samples'},\n                # 'medium_sample': {'max_samples': 1000, 'description': '1K samples'}\n            }\n            \n            for config_name, config in evaluation_configs.items():\n                print(f\"\\nüéØ Running {config['description']} evaluation...\")\n                \n                # Run evaluation\n                results_df, avg_metrics = evaluate_model_comprehensive(\n                    model, tokenizer, all_test_samples, \n                    model_key,\n                    max_samples=config['max_samples'],\n                    generation_config=EVALUATION_CONFIG['full_dataset']['generation_config'],\n                    save_results=True\n                )\n                \n                # Store results\n                result_key = f\"{model_key}_{config_name}\"\n                all_model_results[result_key] = {\n                    'results_df': results_df,\n                    'avg_metrics': avg_metrics,\n                    'model_name': model_key,\n                    'config_name': config_name,\n                    'sample_size': len(results_df)\n                }\n                \n                # Add to summary\n                evaluation_summary.append({\n                    'model': model_key,\n                    'config': config_name,\n                    'samples': len(results_df),\n                    'bleu': avg_metrics['bleu'],\n                    'rouge_l': avg_metrics['rouge_l'],\n                    'rouge_1': avg_metrics['rouge_1'],\n                    'edit_similarity': avg_metrics['edit_similarity'],\n                    'overall_avg': avg_metrics.mean()\n                })\n                \n                # Run detailed analysis\n                print(f\"\\nüìä Running detailed analysis...\")\n                analysis_results = analyze_results_comprehensive(results_df, model_key)\n                \n                # Context length analysis\n                context_analysis = analyze_context_length_performance(results_df, model_key)\n                \n                # Compare with paper\n                comparison_df = compare_with_paper_results(results_df, model_key)\n                \n                # Generate report\n                report_file = generate_evaluation_report(\n                    results_df, model_key, analysis_results, comparison_df\n                )\n                \n                print(f\"‚úÖ {config['description']} evaluation complete!\")\n                \n                # Memory cleanup between configurations\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n            \n            # Clear model from memory before loading next one\n            print(f\"üßπ Clearing {model_key} from memory...\")\n            clear_model_memory(model)\n            \n        except Exception as e:\n            print(f\"‚ùå Error evaluating {model_key}: {e}\")\n            continue\n    \n    # Final summary\n    if evaluation_summary:\n        print(f\"\\n{'='*80}\")\n        print(\"üéâ FULL EVALUATION COMPLETE - FINAL SUMMARY\")\n        print(f\"{'='*80}\")\n        \n        summary_df = pd.DataFrame(evaluation_summary)\n        print(summary_df.round(4))\n        \n        # Save final summary\n        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n        summary_filename = f\"full_evaluation_summary_{timestamp}.csv\"\n        summary_df.to_csv(summary_filename, index=False)\n        print(f\"üíæ Final summary saved to: {summary_filename}\")\n        \n        # Best performing model\n        best_model = summary_df.loc[summary_df['overall_avg'].idxmax()]\n        print(f\"\\nüèÜ Best Performing Model:\")\n        print(f\"   Model: {best_model['model']}\")\n        print(f\"   Configuration: {best_model['config']}\")\n        print(f\"   Overall Score: {best_model['overall_avg']:.4f}\")\n        print(f\"   Sample Size: {best_model['samples']:,}\")\n        \n    else:\n        print(\"‚ùå No evaluations completed successfully\")\n    \n    return all_model_results, evaluation_summary\n\n# EXECUTION CELL - RUN FULL EVALUATION\nprint(\"üî• READY TO RUN FULL DATASET EVALUATION\")\nprint(\"=\" * 50)\nprint(\"‚ö° This will evaluate models on the complete dataset!\")\nprint(f\"üìä Sample count: {len(all_test_samples) if 'all_test_samples' in locals() else 'Not loaded'}\")\nprint(\"‚è±Ô∏è  Estimated time: Several hours for complete evaluation\")\nprint(\"üíæ Results will be saved automatically\")\nprint()\nprint(\"üöÄ To start evaluation, run:\")\nprint(\"   all_results, eval_summary = run_full_evaluation_suite()\")\nprint()\nprint(\"‚ö†Ô∏è  Make sure you have:\")\nprint(\"   ‚Ä¢ Sufficient GPU memory (8GB+ recommended)\")\nprint(\"   ‚Ä¢ Stable internet connection for model downloads\") \nprint(\"   ‚Ä¢ Enough disk space for results (1GB+)\")\n\n# Uncomment the line below to start evaluation automatically\n# all_results, eval_summary = run_full_evaluation_suite()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:09:31.800705Z","iopub.execute_input":"2025-11-14T19:09:31.801324Z","iopub.status.idle":"2025-11-14T19:09:31.817679Z","shell.execute_reply.started":"2025-11-14T19:09:31.801297Z","shell.execute_reply":"2025-11-14T19:09:31.816605Z"}},"outputs":[{"name":"stdout","text":"üî• READY TO RUN FULL DATASET EVALUATION\n==================================================\n‚ö° This will evaluate models on the complete dataset!\nüìä Sample count: Not loaded\n‚è±Ô∏è  Estimated time: Several hours for complete evaluation\nüíæ Results will be saved automatically\n\nüöÄ To start evaluation, run:\n   all_results, eval_summary = run_full_evaluation_suite()\n\n‚ö†Ô∏è  Make sure you have:\n   ‚Ä¢ Sufficient GPU memory (8GB+ recommended)\n   ‚Ä¢ Stable internet connection for model downloads\n   ‚Ä¢ Enough disk space for results (1GB+)\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"# FAST Multi-Repository Detection for CodeRepoQA Structure\ndef detect_repositories_fast():\n    \"\"\"\n    Fast detection based on the actual structure:\n    RepoName/cloudide/workspace/QA_data/RepoName/*.json\n    \"\"\"\n    print(\"üöÄ FAST Multi-Repository Detection for CodeRepoQA\")\n    print(\"=\" * 60)\n    \n    base_path = \"/kaggle/input/coderepoqa\"\n    \n    if not os.path.exists(base_path):\n        print(f\"‚ùå Base path not found: {base_path}\")\n        return {}\n    \n    repositories = {}\n    \n    # Get all repository directories\n    repo_dirs = [d for d in os.listdir(base_path) \n                 if os.path.isdir(os.path.join(base_path, d))]\n    \n    print(f\"üìÇ Found {len(repo_dirs)} potential repositories\")\n    \n    for repo_name in sorted(repo_dirs):\n        repo_path = os.path.join(base_path, repo_name)\n        \n        # Check for the expected structure: cloudide/workspace/QA_data/RepoName/\n        qa_data_path = os.path.join(repo_path, \"cloudide\", \"workspace\", \"QA_data\")\n        \n        if os.path.exists(qa_data_path):\n            # Look for the nested folder named after the repository\n            nested_repo_path = os.path.join(qa_data_path, repo_name)\n            \n            if os.path.exists(nested_repo_path):\n                # Count JSON files in the nested repository folder\n                json_files = []\n                try:\n                    for file in os.listdir(nested_repo_path):\n                        if file.endswith('.json'):\n                            json_files.append(os.path.join(nested_repo_path, file))\n                    \n                    if json_files:\n                        repositories[repo_name] = {\n                            'path': repo_path,\n                            'qa_data_path': qa_data_path,\n                            'nested_repo_path': nested_repo_path,\n                            'json_files': json_files,\n                            'file_count': len(json_files)\n                        }\n                        print(f\"‚úÖ {repo_name}: {len(json_files)} JSON files\")\n                    else:\n                        print(f\"‚ö†Ô∏è  {repo_name}: Nested folder exists but no JSON files found\")\n                except Exception as e:\n                    print(f\"‚ùå {repo_name}: Error reading nested folder - {e}\")\n            else:\n                # Fallback: check if JSON files are directly in QA_data\n                json_files = []\n                try:\n                    for file in os.listdir(qa_data_path):\n                        if file.endswith('.json'):\n                            json_files.append(os.path.join(qa_data_path, file))\n                    \n                    if json_files:\n                        repositories[repo_name] = {\n                            'path': repo_path,\n                            'qa_data_path': qa_data_path,\n                            'nested_repo_path': qa_data_path,  # Same as qa_data_path in this case\n                            'json_files': json_files,\n                            'file_count': len(json_files)\n                        }\n                        print(f\"‚úÖ {repo_name}: {len(json_files)} JSON files (direct in QA_data)\")\n                    else:\n                        print(f\"‚ùå {repo_name}: No nested folder '{repo_name}' found in QA_data\")\n                except Exception as e:\n                    print(f\"‚ùå {repo_name}: Error reading QA_data - {e}\")\n        else:\n            print(f\"‚ùå {repo_name}: No QA_data structure found\")\n    \n    print(f\"\\nüéØ DETECTION COMPLETE\")\n    print(f\"üìä Successfully detected: {len(repositories)} repositories\")\n    print(f\"üìÅ Total JSON files: {sum(repo['file_count'] for repo in repositories.values())}\")\n    \n    return repositories\n\n# Test the fast detection\nprint(\"Testing fast repository detection...\")\ndetected_repos = detect_repositories_fast()\n\nif detected_repos:\n    print(f\"\\nüéâ SUCCESS! Found {len(detected_repos)} repositories:\")\n    for repo_name, info in list(detected_repos.items())[:5]:  # Show first 5\n        print(f\"   üìÅ {repo_name}: {info['file_count']} files\")\n    if len(detected_repos) > 5:\n        print(f\"   ... and {len(detected_repos) - 5} more repositories\")\nelse:\n    print(\"\\n‚ùå No repositories detected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:09:38.900229Z","iopub.execute_input":"2025-11-14T19:09:38.901103Z","iopub.status.idle":"2025-11-14T19:09:39.611114Z","shell.execute_reply.started":"2025-11-14T19:09:38.901076Z","shell.execute_reply":"2025-11-14T19:09:39.610437Z"}},"outputs":[{"name":"stdout","text":"Testing fast repository detection...\nüöÄ FAST Multi-Repository Detection for CodeRepoQA\n============================================================\nüìÇ Found 28 potential repositories\n‚úÖ AutoGPT: 2229 JSON files\n‚úÖ Pillow: 2976 JSON files\n‚úÖ PyMySQL: 660 JSON files\n‚úÖ TypeScript: 33607 JSON files\n‚úÖ angular: 25902 JSON files\n‚úÖ ansible: 31399 JSON files\n‚úÖ core: 50540 JSON files\n‚úÖ dubbo: 6934 JSON files\n‚úÖ fastapi: 3415 JSON files\n‚úÖ guava: 3342 JSON files\n‚úÖ kubernetes: 44567 JSON files\n‚úÖ moby: 21607 JSON files\n‚úÖ nest: 5254 JSON files\n‚úÖ nltk: 1775 JSON files\n‚úÖ node: 17004 JSON files\n‚úÖ numpy: 12076 JSON files\n‚úÖ pandas: 25055 JSON files\n‚úÖ plotly.py: 2829 JSON files\n‚úÖ py-tree-sitter: 155 JSON files\n‚úÖ pytorch: 42408 JSON files\n‚úÖ rich: 1287 JSON files\n‚úÖ scipy: 9775 JSON files\n‚úÖ spring-framework: 24516 JSON files\n‚úÖ terraform: 20090 JSON files\n‚úÖ transformers: 15052 JSON files\n‚úÖ typeorm: 7828 JSON files\n‚úÖ vscode: 148293 JSON files\n‚úÖ vue: 9744 JSON files\n\nüéØ DETECTION COMPLETE\nüìä Successfully detected: 28 repositories\nüìÅ Total JSON files: 570319\n\nüéâ SUCCESS! Found 28 repositories:\n   üìÅ AutoGPT: 2229 files\n   üìÅ Pillow: 2976 files\n   üìÅ PyMySQL: 660 files\n   üìÅ TypeScript: 33607 files\n   üìÅ angular: 25902 files\n   ... and 23 more repositories\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"def run_complete_multi_repository_evaluation(\n    repositories, \n    models_to_test=None, \n    samples_per_repo=50,\n    max_repos=None,\n    save_results=True\n):\n    \"\"\"\n    Complete multi-repository evaluation exactly like the original CodeRepoQA paper.\n    \n    Args:\n        repositories: Dictionary from detect_repositories_fast()\n        models_to_test: List of model names to test (default: all 4 models)\n        samples_per_repo: Number of samples to evaluate per repository\n        max_repos: Maximum number of repositories to test (for quick testing)\n        save_results: Whether to save detailed results to files\n    \"\"\"\n    \n    if models_to_test is None:\n        models_to_test = [\n            \"deepseek-ai/deepseek-coder-1.3b-instruct\",\n            \"Qwen/CodeQwen1.5-7B-Chat\", \n            \"codellama/CodeLlama-7b-Instruct-hf\",\n            \"mistralai/Mistral-7B-Instruct-v0.1\"\n        ]\n    \n    print(\"üöÄ COMPLETE MULTI-REPOSITORY EVALUATION\")\n    print(\"=\" * 70)\n    print(f\"üìä Repositories: {len(repositories)}\")\n    print(f\"ü§ñ Models: {len(models_to_test)}\")\n    print(f\"üìù Samples per repo: {samples_per_repo}\")\n    print(f\"üéØ Total evaluations: {len(repositories) * len(models_to_test) * samples_per_repo}\")\n    print(\"=\" * 70)\n    \n    # Repository list (limit if specified)\n    repo_list = list(repositories.keys())\n    if max_repos:\n        repo_list = repo_list[:max_repos]\n        print(f\"‚ö° Quick test mode: Testing only first {max_repos} repositories\")\n    \n    # Results storage\n    all_results = {}\n    repository_summaries = {}\n    \n    # Progress tracking\n    total_combinations = len(repo_list) * len(models_to_test)\n    current_combination = 0\n    \n    for repo_name in repo_list:\n        print(f\"\\nüîç REPOSITORY: {repo_name}\")\n        print(f\"üìÅ Files: {repositories[repo_name]['file_count']} JSON files\")\n        print(\"-\" * 50)\n        \n        # Load repository data\n        try:\n            repo_samples = load_repository_samples(repo_name, repositories[repo_name], limit=samples_per_repo)\n            if not repo_samples:\n                print(f\"‚ùå No samples found for {repo_name}\")\n                continue\n                \n            print(f\"‚úÖ Loaded {len(repo_samples)} samples from {repo_name}\")\n        except Exception as e:\n            print(f\"‚ùå Error loading {repo_name}: {str(e)}\")\n            continue\n        \n        repository_results = {}\n        \n        for model_name in models_to_test:\n            current_combination += 1\n            model_short = model_name.split('/')[-1]\n            \n            print(f\"\\nü§ñ Model {current_combination}/{total_combinations}: {model_short}\")\n            print(f\"üìä Repository: {repo_name}\")\n            \n            try:\n                # Load model and tokenizer\n                model, tokenizer = load_model_with_config(model_name)\n                \n                # Run evaluation using the existing function - FIXED PARAMETER NAME\n                results_df, avg_metrics = evaluate_model_comprehensive(\n                    model=model,\n                    tokenizer=tokenizer, \n                    samples=repo_samples,\n                    model_name=model_short,\n                    max_samples=samples_per_repo,  # FIXED: was num_samples, should be max_samples\n                    generation_config={'max_new_tokens': 512, 'temperature': 0.7},\n                    save_results=False  # We'll save at the end\n                )\n                \n                # Store results\n                repository_results[model_name] = {\n                    'results_df': results_df,\n                    'aggregate_scores': {\n                        'bleu': avg_metrics['bleu'],\n                        'rouge_l': avg_metrics['rouge_l'],\n                        'rouge_1': avg_metrics['rouge_1'],\n                        'edit_similarity': avg_metrics['edit_similarity']\n                    },\n                    'sample_count': len(results_df),\n                    'success_rate': (len(results_df) - (results_df[['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']] == 0).all(axis=1).sum()) / len(results_df)\n                }\n                \n                # Print summary\n                print(f\"   ‚úÖ BLEU: {avg_metrics['bleu']:.3f}\")\n                print(f\"   ‚úÖ ROUGE-L: {avg_metrics['rouge_l']:.3f}\")\n                print(f\"   ‚úÖ Edit Sim: {avg_metrics['edit_similarity']:.3f}\")\n                \n                # Clean up GPU memory\n                del model, tokenizer\n                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n                    \n            except Exception as e:\n                print(f\"   ‚ùå Error: {str(e)}\")\n                repository_results[model_name] = None\n        \n        all_results[repo_name] = repository_results\n        \n        # Calculate repository summary\n        repo_summary = calculate_repository_summary(repository_results)\n        repository_summaries[repo_name] = repo_summary\n        \n        print(f\"\\nüìà REPOSITORY SUMMARY: {repo_name}\")\n        if repo_summary:\n            print(f\"   üèÜ Best BLEU: {repo_summary['best_bleu']['model']} ({repo_summary['best_bleu']['score']:.3f})\")\n            print(f\"   üèÜ Best ROUGE-L: {repo_summary['best_rouge_l']['model']} ({repo_summary['best_rouge_l']['score']:.3f})\")\n            print(f\"   üìä Avg BLEU: {repo_summary['avg_bleu']:.3f}\")\n            print(f\"   üìä Avg ROUGE-L: {repo_summary['avg_rouge_l']:.3f}\")\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"üéØ COMPLETE MULTI-REPOSITORY EVALUATION FINISHED\")\n    print(\"=\" * 70)\n    \n    # Generate cross-repository analysis\n    cross_repo_analysis = generate_cross_repository_analysis(all_results)\n    \n    # Generate final comprehensive report\n    final_report = generate_comprehensive_report(all_results, repository_summaries, cross_repo_analysis)\n    \n    if save_results:\n        # Save results to files\n        import json\n        import pandas as pd\n        from datetime import datetime\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        \n        # Save raw results (convert DataFrames to dicts for JSON serialization)\n        serializable_results = {}\n        for repo_name, repo_results in all_results.items():\n            serializable_results[repo_name] = {}\n            for model_name, results in repo_results.items():\n                if results:\n                    serializable_results[repo_name][model_name] = {\n                        'aggregate_scores': results['aggregate_scores'],\n                        'sample_count': results['sample_count'],\n                        'success_rate': results['success_rate']\n                        # Note: results_df not included to avoid size issues\n                    }\n                else:\n                    serializable_results[repo_name][model_name] = None\n        \n        with open(f'multi_repo_results_{timestamp}.json', 'w') as f:\n            json.dump(serializable_results, f, indent=2, default=str)\n        \n        # Save summary as CSV\n        summary_df = pd.DataFrame(repository_summaries).T\n        summary_df.to_csv(f'repository_summary_{timestamp}.csv')\n        \n        print(f\"\\nüíæ Results saved:\")\n        print(f\"   üìÑ Raw results: multi_repo_results_{timestamp}.json\")\n        print(f\"   üìä Summary: repository_summary_{timestamp}.csv\")\n    \n    return {\n        'all_results': all_results,\n        'repository_summaries': repository_summaries,\n        'cross_repository_analysis': cross_repo_analysis,\n        'final_report': final_report\n    }\n\ndef load_repository_samples(repo_name, repo_info, limit=None):\n    \"\"\"Load samples from a specific repository using the detected repository info.\"\"\"\n    \n    qa_data_path = repo_info['qa_data_path']\n    json_files = repo_info['json_files']\n    \n    print(f\"   üìÇ Loading from: {qa_data_path}\")\n    print(f\"   üìÑ Processing {len(json_files)} JSON files\")\n    \n    samples = []\n    \n    # Load samples from JSON files\n    for json_file in json_files:\n        try:\n            with open(json_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                \n            # Handle different JSON structures\n            if isinstance(data, list):\n                samples.extend(data)\n            elif isinstance(data, dict):\n                samples.append(data)\n                \n        except Exception as e:\n            print(f\"   ‚ö†Ô∏è Error loading {json_file}: {str(e)}\")\n            continue\n    \n    if not samples:\n        print(f\"   ‚ùå No samples found for repository: {repo_name}\")\n        return []\n    \n    print(f\"   ‚úÖ Loaded {len(samples)} total samples\")\n    \n    # Limit samples if requested\n    if limit and len(samples) > limit:\n        # Use stratified sampling for better representation\n        indices = np.linspace(0, len(samples)-1, limit, dtype=int)\n        samples = [samples[i] for i in indices]\n        print(f\"   üéØ Selected {len(samples)} samples (stratified sampling)\")\n    \n    return samples\n\ndef calculate_repository_summary(repository_results):\n    \"\"\"Calculate summary statistics for a single repository across all models.\"\"\"\n    if not repository_results:\n        return None\n    \n    summary = {\n        'models_tested': 0,\n        'successful_models': 0,\n        'avg_bleu': 0,\n        'avg_rouge_l': 0,\n        'avg_rouge_1': 0,\n        'avg_edit_similarity': 0,\n        'best_bleu': {'model': '', 'score': 0},\n        'best_rouge_l': {'model': '', 'score': 0},\n        'model_rankings': {}\n    }\n    \n    valid_results = []\n    for model_name, results in repository_results.items():\n        summary['models_tested'] += 1\n        \n        if results and 'aggregate_scores' in results:\n            summary['successful_models'] += 1\n            scores = results['aggregate_scores']\n            valid_results.append({\n                'model': model_name.split('/')[-1],\n                'bleu': scores.get('bleu', 0),\n                'rouge_l': scores.get('rouge_l', 0),\n                'rouge_1': scores.get('rouge_1', 0),\n                'edit_similarity': scores.get('edit_similarity', 0)\n            })\n    \n    if valid_results:\n        # Calculate averages\n        summary['avg_bleu'] = sum(r['bleu'] for r in valid_results) / len(valid_results)\n        summary['avg_rouge_l'] = sum(r['rouge_l'] for r in valid_results) / len(valid_results)\n        summary['avg_rouge_1'] = sum(r['rouge_1'] for r in valid_results) / len(valid_results)\n        summary['avg_edit_similarity'] = sum(r['edit_similarity'] for r in valid_results) / len(valid_results)\n        \n        # Find best performers\n        best_bleu = max(valid_results, key=lambda x: x['bleu'])\n        summary['best_bleu'] = {'model': best_bleu['model'], 'score': best_bleu['bleu']}\n        \n        best_rouge = max(valid_results, key=lambda x: x['rouge_l'])\n        summary['best_rouge_l'] = {'model': best_rouge['model'], 'score': best_rouge['rouge_l']}\n        \n        # Model rankings\n        for result in valid_results:\n            summary['model_rankings'][result['model']] = {\n                'bleu': result['bleu'],\n                'rouge_l': result['rouge_l'],\n                'rouge_1': result['rouge_1'],\n                'edit_similarity': result['edit_similarity']\n            }\n    \n    return summary\n\ndef generate_cross_repository_analysis(all_results):\n    \"\"\"Generate cross-repository analysis to understand model consistency.\"\"\"\n    \n    analysis = {\n        'model_consistency': {},\n        'repository_difficulty': {},\n        'overall_rankings': {},\n        'statistical_significance': {}\n    }\n    \n    # Get all model names\n    model_names = set()\n    for repo_results in all_results.values():\n        model_names.update(repo_results.keys())\n    \n    # Analyze each model's consistency across repositories\n    for model_name in model_names:\n        model_short = model_name.split('/')[-1]\n        model_scores = {'bleu': [], 'rouge_l': [], 'rouge_1': [], 'edit_similarity': []}\n        successful_repos = 0\n        \n        for repo_name, repo_results in all_results.items():\n            if model_name in repo_results and repo_results[model_name]:\n                if 'aggregate_scores' in repo_results[model_name]:\n                    scores = repo_results[model_name]['aggregate_scores']\n                    model_scores['bleu'].append(scores.get('bleu', 0))\n                    model_scores['rouge_l'].append(scores.get('rouge_l', 0))\n                    model_scores['rouge_1'].append(scores.get('rouge_1', 0))\n                    model_scores['edit_similarity'].append(scores.get('edit_similarity', 0))\n                    successful_repos += 1\n        \n        if model_scores['bleu']:  # If we have any scores\n            import numpy as np\n            analysis['model_consistency'][model_short] = {\n                'successful_repositories': successful_repos,\n                'total_repositories': len(all_results),\n                'success_rate': successful_repos / len(all_results),\n                'bleu_stats': {\n                    'mean': np.mean(model_scores['bleu']),\n                    'std': np.std(model_scores['bleu']),\n                    'min': np.min(model_scores['bleu']),\n                    'max': np.max(model_scores['bleu'])\n                },\n                'rouge_l_stats': {\n                    'mean': np.mean(model_scores['rouge_l']),\n                    'std': np.std(model_scores['rouge_l']),\n                    'min': np.min(model_scores['rouge_l']),\n                    'max': np.max(model_scores['rouge_l'])\n                }\n            }\n    \n    # Analyze repository difficulty\n    for repo_name, repo_results in all_results.items():\n        repo_scores = []\n        for model_name, results in repo_results.items():\n            if results and 'aggregate_scores' in results:\n                scores = results['aggregate_scores']\n                repo_scores.append({\n                    'bleu': scores.get('bleu', 0),\n                    'rouge_l': scores.get('rouge_l', 0)\n                })\n        \n        if repo_scores:\n            import numpy as np\n            analysis['repository_difficulty'][repo_name] = {\n                'models_tested': len(repo_scores),\n                'avg_bleu': np.mean([s['bleu'] for s in repo_scores]),\n                'avg_rouge_l': np.mean([s['rouge_l'] for s in repo_scores]),\n                'difficulty_rank': 0  # Will be calculated later\n            }\n    \n    # Calculate difficulty rankings\n    if analysis['repository_difficulty']:\n        sorted_repos = sorted(\n            analysis['repository_difficulty'].items(),\n            key=lambda x: (x[1]['avg_bleu'] + x[1]['avg_rouge_l']) / 2\n        )\n        \n        for i, (repo_name, stats) in enumerate(sorted_repos):\n            analysis['repository_difficulty'][repo_name]['difficulty_rank'] = i + 1\n    \n    return analysis\n\ndef generate_comprehensive_report(all_results, repository_summaries, cross_repo_analysis):\n    \"\"\"Generate a comprehensive report like the original paper.\"\"\"\n    \n    report = {\n        'executive_summary': {},\n        'model_performance': {},\n        'repository_analysis': {},\n        'key_findings': [],\n        'recommendations': []\n    }\n    \n    # Executive Summary\n    total_repos = len(all_results)\n    total_models = len(set(model for repo in all_results.values() for model in repo.keys()))\n    \n    report['executive_summary'] = {\n        'repositories_evaluated': total_repos,\n        'models_evaluated': total_models,\n        'total_evaluations': sum(len(repo) for repo in all_results.values()),\n        'evaluation_scope': 'Multi-repository evaluation following CodeRepoQA paper methodology'\n    }\n    \n    # Model Performance Summary\n    if cross_repo_analysis['model_consistency']:\n        model_rankings = []\n        for model, stats in cross_repo_analysis['model_consistency'].items():\n            model_rankings.append({\n                'model': model,\n                'avg_bleu': stats['bleu_stats']['mean'],\n                'avg_rouge_l': stats['rouge_l_stats']['mean'],\n                'consistency_bleu': 1 / (stats['bleu_stats']['std'] + 0.001),  # Lower std = higher consistency\n                'success_rate': stats['success_rate']\n            })\n        \n        # Sort by average performance\n        model_rankings.sort(key=lambda x: (x['avg_bleu'] + x['avg_rouge_l']) / 2, reverse=True)\n        report['model_performance']['rankings'] = model_rankings\n    \n    # Key Findings\n    if model_rankings:\n        best_model = model_rankings[0]\n        report['key_findings'].append(f\"Best overall model: {best_model['model']} (BLEU: {best_model['avg_bleu']:.3f}, ROUGE-L: {best_model['avg_rouge_l']:.3f})\")\n    \n    if cross_repo_analysis['repository_difficulty']:\n        easiest_repo = min(cross_repo_analysis['repository_difficulty'].items(), \n                          key=lambda x: x[1]['difficulty_rank'])\n        hardest_repo = max(cross_repo_analysis['repository_difficulty'].items(), \n                          key=lambda x: x[1]['difficulty_rank'])\n        \n        report['key_findings'].append(f\"Easiest repository: {easiest_repo[0]} (avg BLEU: {easiest_repo[1]['avg_bleu']:.3f})\")\n        report['key_findings'].append(f\"Hardest repository: {hardest_repo[0]} (avg BLEU: {hardest_repo[1]['avg_bleu']:.3f})\")\n    \n    return report\n\nprint(\"‚úÖ Multi-repository evaluation functions loaded successfully!\")\nprint(\"üöÄ Ready to run complete evaluation like the original paper!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:09:48.277750Z","iopub.execute_input":"2025-11-14T19:09:48.278431Z","iopub.status.idle":"2025-11-14T19:09:48.341025Z","shell.execute_reply.started":"2025-11-14T19:09:48.278404Z","shell.execute_reply":"2025-11-14T19:09:48.340229Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Multi-repository evaluation functions loaded successfully!\nüöÄ Ready to run complete evaluation like the original paper!\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# # FIX: Model Configuration and Missing Functions\n# print(\"üîß Applying fixes for model loading and evaluation functions...\")\n\n# def process_raw_github_issue_to_sample(raw_issue):\n#     \"\"\"\n#     Convert raw GitHub issue JSON to a processed sample with context and ground truth\n#     \"\"\"\n#     try:\n#         # Build conversation from issue and comments\n#         conversation = []\n        \n#         # 1. Add the initial issue\n#         issue_title = raw_issue.get('title', '').strip()\n#         issue_body = raw_issue.get('body', '').strip()\n        \n#         if issue_title and issue_body:\n#             issue_content = f\"Title: {issue_title}\\n\\n{issue_body}\"\n#         elif issue_title:\n#             issue_content = f\"Title: {issue_title}\"\n#         elif issue_body:\n#             issue_content = issue_body\n#         else:\n#             issue_content = \"No content available\"\n        \n#         conversation.append({\n#             'speaker': 'user',\n#             'content': issue_content,\n#             'role': 'USER'\n#         })\n        \n#         # 2. Add comments if available\n#         comments_details = raw_issue.get('comments_details', [])\n#         maintainer_responses = []\n        \n#         for comment in comments_details:\n#             comment_body = comment.get('body', '').strip()\n#             if not comment_body or len(comment_body) < 10:\n#                 continue\n                \n#             # Determine if this is a maintainer response\n#             author_association = comment.get('author_association', 'NONE')\n#             is_maintainer = author_association in ['OWNER', 'MEMBER', 'COLLABORATOR']\n            \n#             conversation.append({\n#                 'speaker': 'maintainer' if is_maintainer else 'user',\n#                 'content': comment_body,\n#                 'role': author_association\n#             })\n            \n#             if is_maintainer:\n#                 maintainer_responses.append(len(conversation) - 1)\n        \n#         # 3. Create test samples - one per maintainer response\n#         samples = []\n#         for maintainer_idx in maintainer_responses:\n#             context_turns = conversation[:maintainer_idx]\n#             ground_truth = conversation[maintainer_idx]['content']\n            \n#             if len(context_turns) >= 1:  # Need at least the initial issue\n#                 samples.append({\n#                     'issue_number': raw_issue.get('number', 'unknown'),\n#                     'context': context_turns,\n#                     'ground_truth': ground_truth,\n#                     'turn_number': maintainer_responses.index(maintainer_idx) + 1,\n#                     'total_conversation_turns': len(conversation),\n#                     'maintainer_role': conversation[maintainer_idx]['role'],\n#                     'total_maintainer_turns': len(maintainer_responses)\n#                 })\n        \n#         return samples if samples else []\n        \n#     except Exception as e:\n#         print(f\"‚ö†Ô∏è Error processing issue {raw_issue.get('number', 'unknown')}: {e}\")\n#         return []\n\n# # Add missing batch_generate_responses function with correct signature for samples\n# def batch_generate_responses(model, tokenizer, samples, generation_config=None, batch_size=None):\n#     \"\"\"\n#     Generate responses for a batch of samples (handles both raw issues and processed samples)\n#     \"\"\"\n#     if generation_config is None:\n#         generation_config = {\n#             'max_new_tokens': 512,\n#             'do_sample': True,\n#             'pad_token_id': tokenizer.eos_token_id\n#         }\n    \n#     # Convert samples to prompts\n#     prompts = []\n#     processed_samples = []\n    \n#     for sample in samples:\n#         if isinstance(sample, dict):\n#             # Check if this is a processed sample or raw GitHub issue\n#             if 'context' in sample and 'ground_truth' in sample:\n#                 # Already processed sample\n#                 prompt = format_conversation_context(sample['context'])\n#                 processed_samples.append(sample)\n#             elif 'title' in sample and 'body' in sample:\n#                 # Raw GitHub issue - process it first\n#                 print(\"üîÑ Processing raw GitHub issue into samples...\")\n#                 issue_samples = process_raw_github_issue_to_sample(sample)\n#                 if issue_samples:\n#                     # Use the first sample from this issue\n#                     first_sample = issue_samples[0]\n#                     prompt = format_conversation_context(first_sample['context'])\n#                     processed_samples.append(first_sample)\n#                 else:\n#                     # Fallback: use issue title and body\n#                     title = sample.get('title', '')\n#                     body = sample.get('body', '')\n#                     prompt = f\"User: Title: {title}\\n\\n{body}\\n\\nAssistant:\"\n#                     processed_samples.append({\n#                         'issue_number': sample.get('number', 'unknown'),\n#                         'context': [{'speaker': 'user', 'content': f\"Title: {title}\\n\\n{body}\", 'role': 'USER'}],\n#                         'ground_truth': \"No maintainer response available\"\n#                     })\n#             else:\n#                 # Unknown format - convert to string\n#                 prompt = str(sample)\n#                 processed_samples.append({'context': [], 'ground_truth': 'Unknown format'})\n#         else:\n#             # String prompt\n#             prompt = str(sample)\n#             processed_samples.append({'context': [], 'ground_truth': 'String input'})\n        \n#         prompts.append(prompt)\n    \n#     # Generate responses\n#     if batch_size and len(prompts) > batch_size:\n#         all_responses = []\n#         for i in range(0, len(prompts), batch_size):\n#             batch_prompts = prompts[i:i+batch_size]\n#             batch_responses = batch_generate_responses_internal(model, tokenizer, batch_prompts, generation_config)\n#             all_responses.extend(batch_responses)\n#         return all_responses\n    \n#     return batch_generate_responses_internal(model, tokenizer, prompts, generation_config)\n\n# def batch_generate_responses_internal(model, tokenizer, prompts, generation_config):\n#     \"\"\"\n#     Internal function to generate responses for string prompts\n#     \"\"\"\n#     # Clean generation config for compatibility\n#     clean_config = generation_config.copy()\n#     if 'pad_token_id' in clean_config:\n#         del clean_config['pad_token_id']  # Will be set automatically\n    \n#     # Tokenize inputs\n#     inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\", max_length=2048)\n    \n#     # Move to model device\n#     if torch.cuda.is_available() and hasattr(model, 'device'):\n#         inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n#     # Generate responses\n#     with torch.no_grad():\n#         try:\n#             outputs = model.generate(\n#                 input_ids=inputs['input_ids'],\n#                 attention_mask=inputs['attention_mask'],\n#                 pad_token_id=tokenizer.eos_token_id,\n#                 **clean_config\n#             )\n#         except Exception as e:\n#             print(f\"‚ö†Ô∏è Generation error: {e}\")\n#             return [\"Error in generation\" for _ in prompts]\n    \n#     # Decode responses (remove input tokens)\n#     responses = []\n#     for i, output in enumerate(outputs):\n#         # Get only the new tokens (response)\n#         input_length = inputs['input_ids'][i].shape[0]\n#         response_tokens = output[input_length:]\n#         response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n#         responses.append(response.strip())\n    \n#     return responses\n\n# def format_conversation_context(context, max_context_length=4000):\n#     \"\"\"\n#     Format conversation context for model input with length management\n#     \"\"\"\n#     formatted = \"\"\n#     total_length = 0\n    \n#     # Add context turns, respecting length limits\n#     for i, turn in enumerate(context):\n#         turn_text = \"\"\n#         if turn['speaker'] == 'user':\n#             turn_text = f\"User: {turn['content']}\\n\\n\"\n#         else:\n#             turn_text = f\"Assistant: {turn['content']}\\n\\n\"\n        \n#         # Check if adding this turn would exceed limit\n#         if total_length + len(turn_text) > max_context_length and i > 0:\n#             # If we must truncate, ensure we keep at least the first turn (issue description)\n#             if i == 1:  # First turn must be preserved\n#                 formatted += turn_text[:max_context_length - total_length - 50] + \"...\\n\\n\"\n#             break\n        \n#         formatted += turn_text\n#         total_length += len(turn_text)\n    \n#     # Add assistant prompt\n#     formatted += \"Assistant:\"\n#     return formatted\n\n# # Model name mapping for correct HuggingFace identifiers\n# MODEL_NAME_MAP = {\n#     'deepseek-coder-6.7b': 'deepseek-ai/deepseek-coder-6.7b-instruct',\n#     'codellama-7b-instruct': 'codellama/CodeLlama-7b-Instruct-hf',\n#     'codeqwen-7b-chat': 'Qwen/CodeQwen1.5-7B-Chat',\n#     'mistral-7b': 'mistralai/Mistral-7B-Instruct-v0.1'\n# }\n\n# # Override the load_model_with_config function to use correct model names\n# def load_model_with_config_fixed(model_name, max_memory_gb=None):\n#     \"\"\"\n#     Load a model with optimized configuration for evaluation (FIXED VERSION)\n#     \"\"\"\n#     print(f\"üîÑ Loading model: {model_name}\")\n    \n#     try:\n#         # Check if model_name is a key in our mapping\n#         if model_name in MODEL_NAME_MAP:\n#             actual_model_name = MODEL_NAME_MAP[model_name]\n#             print(f\"üìù Using model: {actual_model_name}\")\n#         else:\n#             actual_model_name = model_name\n        \n#         # Configure device mapping for multi-GPU setups\n#         device_map = \"auto\" if torch.cuda.device_count() > 1 else None\n        \n#         # Memory optimization\n#         torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n        \n#         # Load tokenizer\n#         tokenizer = AutoTokenizer.from_pretrained(\n#             actual_model_name,\n#             trust_remote_code=True,\n#             padding_side=\"left\"  # Important for batch generation\n#         )\n        \n#         # Add pad token if missing\n#         if tokenizer.pad_token is None:\n#             tokenizer.pad_token = tokenizer.eos_token\n            \n#         # Load model with optimizations\n#         model = AutoModelForCausalLM.from_pretrained(\n#             actual_model_name,\n#             torch_dtype=torch_dtype,\n#             device_map=device_map,\n#             trust_remote_code=True,\n#             load_in_8bit=False,  # Set to True if you need memory savings\n#             low_cpu_mem_usage=True\n#         )\n        \n#         print(f\"‚úÖ Model loaded successfully\")\n#         print(f\"üìä Model parameters: {model.num_parameters():,}\")\n#         if torch.cuda.is_available():\n#             print(f\"üîß Device: {next(model.parameters()).device}\")\n#             print(f\"üíæ Model dtype: {next(model.parameters()).dtype}\")\n        \n#         return model, tokenizer\n        \n#     except Exception as e:\n#         print(f\"‚ùå Error loading model {actual_model_name}: {str(e)}\")\n#         return None, None\n\n# # Override the original function\n# load_model_with_config = load_model_with_config_fixed\n\n# print(\"‚úÖ Fixes applied successfully!\")\n# print(\"üìù Now handles both raw GitHub issues and processed samples\")\n# print(\"üîß Will automatically convert raw issues to proper test samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T18:46:40.880530Z","iopub.execute_input":"2025-11-14T18:46:40.881318Z","iopub.status.idle":"2025-11-14T18:46:40.904485Z","shell.execute_reply.started":"2025-11-14T18:46:40.881292Z","shell.execute_reply":"2025-11-14T18:46:40.903605Z"}},"outputs":[{"name":"stdout","text":"üîß Applying fixes for model loading and evaluation functions...\n‚úÖ Fixes applied successfully!\nüìù Now handles both raw GitHub issues and processed samples\nüîß Will automatically convert raw issues to proper test samples\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# FIX: Generate Comprehensive Report Function\ndef generate_comprehensive_report_fixed(all_results, repository_summaries, cross_repo_analysis):\n    \"\"\"\n    Generate a comprehensive evaluation report (FIXED VERSION)\n    \"\"\"\n    from datetime import datetime\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    \n    report = {\n        'evaluation_metadata': {\n            'timestamp': timestamp,\n            'total_repositories': len(repository_summaries),\n            'total_models': len(set(key.split('_')[0] for key in all_results.keys())) if all_results else 0,\n            'evaluation_type': 'Multi-Repository CodeRepoQA Evaluation'\n        },\n        'repository_summaries': repository_summaries,\n        'cross_repository_analysis': cross_repo_analysis,\n        'model_performance': {},\n        'key_findings': []\n    }\n    \n    # Aggregate model performance across all repositories\n    model_performance = {}\n    for result_key, result_data in all_results.items():\n        model_name = result_key.split('_')[0]\n        if model_name not in model_performance:\n            model_performance[model_name] = {\n                'total_samples': 0,\n                'bleu_scores': [],\n                'rouge_l_scores': [],\n                'rouge_1_scores': [],\n                'edit_sim_scores': [],\n                'repositories_tested': []\n            }\n        \n        if 'results_df' in result_data and result_data['results_df'] is not None:\n            df = result_data['results_df']\n            model_performance[model_name]['total_samples'] += len(df)\n            model_performance[model_name]['bleu_scores'].extend(df['bleu_score'].tolist())\n            model_performance[model_name]['rouge_l_scores'].extend(df['rouge_l'].tolist())\n            model_performance[model_name]['rouge_1_scores'].extend(df['rouge_1'].tolist())\n            model_performance[model_name]['edit_sim_scores'].extend(df['edit_similarity'].tolist())\n            \n            # Extract repository name from result_key\n            repo_name = '_'.join(result_key.split('_')[1:])\n            model_performance[model_name]['repositories_tested'].append(repo_name)\n    \n    # Calculate aggregate statistics\n    for model_name, data in model_performance.items():\n        if data['bleu_scores']:  # Only if we have data\n            data['avg_bleu'] = np.mean(data['bleu_scores'])\n            data['avg_rouge_l'] = np.mean(data['rouge_l_scores'])\n            data['avg_rouge_1'] = np.mean(data['rouge_1_scores'])\n            data['avg_edit_sim'] = np.mean(data['edit_sim_scores'])\n            data['std_bleu'] = np.std(data['bleu_scores'])\n            data['std_rouge_l'] = np.std(data['rouge_l_scores'])\n            data['repositories_count'] = len(set(data['repositories_tested']))\n        else:\n            # No valid data\n            data['avg_bleu'] = 0.0\n            data['avg_rouge_l'] = 0.0\n            data['avg_rouge_1'] = 0.0\n            data['avg_edit_sim'] = 0.0\n            data['std_bleu'] = 0.0\n            data['std_rouge_l'] = 0.0\n            data['repositories_count'] = 0\n    \n    report['model_performance'] = model_performance\n    \n    # Create model rankings - Initialize as empty list first\n    model_rankings = []\n    for model_name, data in model_performance.items():\n        model_rankings.append({\n            'model': model_name,\n            'avg_bleu': data['avg_bleu'],\n            'avg_rouge_l': data['avg_rouge_l'],\n            'total_samples': data['total_samples'],\n            'repositories_tested': data['repositories_count']\n        })\n    \n    # Sort by average BLEU score (only if we have rankings)\n    if model_rankings:\n        model_rankings.sort(key=lambda x: x['avg_bleu'], reverse=True)\n    \n    # Key Findings\n    if model_rankings and len(model_rankings) > 0:\n        best_model = model_rankings[0]\n        report['key_findings'].append(f\"Best overall model: {best_model['model']} (BLEU: {best_model['avg_bleu']:.3f}, ROUGE-L: {best_model['avg_rouge_l']:.3f})\")\n        \n        if len(model_rankings) > 1:\n            worst_model = model_rankings[-1]\n            performance_gap = best_model['avg_bleu'] - worst_model['avg_bleu']\n            report['key_findings'].append(f\"Performance gap: {performance_gap:.3f} BLEU points between best and worst models\")\n    else:\n        report['key_findings'].append(\"No valid evaluation results found across all models and repositories\")\n    \n    report['model_rankings'] = model_rankings\n    \n    return report\n\n# Override the original function\ngenerate_comprehensive_report = generate_comprehensive_report_fixed\n\nprint(\"‚úÖ Fixed generate_comprehensive_report function!\")\nprint(\"üîß All major issues should now be resolved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:10:04.880466Z","iopub.execute_input":"2025-11-14T19:10:04.881304Z","iopub.status.idle":"2025-11-14T19:10:04.894785Z","shell.execute_reply.started":"2025-11-14T19:10:04.881277Z","shell.execute_reply":"2025-11-14T19:10:04.893940Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Fixed generate_comprehensive_report function!\nüîß All major issues should now be resolved\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# üß† Complete Memory-Optimized Model Loading with Safe Error Handling\nimport torch\nimport gc\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory thoroughly\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n        gc.collect()\n\ndef load_model_with_config_memory_optimized(model_name, max_gpu_memory_gb=7.5):\n    \"\"\"\n    Load model with aggressive memory optimization and safe error handling\n    \"\"\"\n    # FIXED: Include short model names in the mapping\n    model_map = {\n        # Short names (used in evaluation)\n        'deepseek-coder-1.3b': 'deepseek-ai/deepseek-coder-1.3b-instruct',\n        'codeqwen-7b': 'Qwen/CodeQwen1.5-7B-Chat',\n        'codellama-7b': 'codellama/CodeLlama-7b-Instruct-hf',\n        'mistral-7b': 'mistralai/Mistral-7B-Instruct-v0.1',\n        \n        # Full HuggingFace names (fallback)\n        'deepseek-ai/deepseek-coder-1.3b-instruct': 'deepseek-ai/deepseek-coder-1.3b-instruct',\n        'Qwen/CodeQwen1.5-7B-Chat': 'Qwen/CodeQwen1.5-7B-Chat', \n        'codellama/CodeLlama-7b-Instruct-hf': 'codellama/CodeLlama-7b-Instruct-hf',\n        'mistralai/Mistral-7B-Instruct-v0.1': 'mistralai/Mistral-7B-Instruct-v0.1'\n    }\n    \n    hf_model_name = model_map.get(model_name, model_name)\n    \n    print(f\"üîÑ Model mapping: {model_name} ‚Üí {hf_model_name}\")\n    \n    # Clear memory before loading\n    clear_gpu_memory()\n    \n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        \n        print(f\"üîÑ Loading tokenizer for {hf_model_name}...\")\n        tokenizer = AutoTokenizer.from_pretrained(hf_model_name, trust_remote_code=True)\n        \n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        \n        print(f\"üîÑ Loading model {hf_model_name} with memory optimization...\")\n        \n        # Calculate max memory in bytes\n        max_memory_bytes = int(max_gpu_memory_gb * 1024**3)\n        \n        # Load with strict memory limits and CPU fallback\n        model = AutoModelForCausalLM.from_pretrained(\n            hf_model_name,\n            torch_dtype=torch.float16,  # Use float16 to save memory\n            device_map=\"auto\",\n            trust_remote_code=True,\n            max_memory={0: f\"{max_gpu_memory_gb}GB\"},\n            low_cpu_mem_usage=True,\n            # Removed quantization to avoid bitsandbytes dependency\n        )\n        \n        print(f\"‚úÖ Successfully loaded {model_name} ({hf_model_name})\")\n        print(f\"üìä Model device: {next(model.parameters()).device}\")\n        \n        # Check GPU memory usage\n        if torch.cuda.is_available():\n            memory_allocated = torch.cuda.memory_allocated() / 1024**3\n            memory_reserved = torch.cuda.memory_reserved() / 1024**3\n            print(f\"üîç GPU Memory - Allocated: {memory_allocated:.2f}GB, Reserved: {memory_reserved:.2f}GB\")\n        \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"‚ùå Failed to load {model_name} ({hf_model_name}): {str(e)}\")\n        print(\"üîÑ Attempting CPU fallback...\")\n        \n        try:\n            # Try CPU fallback\n            model = AutoModelForCausalLM.from_pretrained(\n                hf_model_name,\n                torch_dtype=torch.float16,\n                device_map=\"cpu\",\n                trust_remote_code=True,\n                low_cpu_mem_usage=True\n            )\n            print(f\"‚úÖ Loaded {model_name} ({hf_model_name}) on CPU\")\n            return model, tokenizer\n            \n        except Exception as cpu_error:\n            print(f\"‚ùå CPU fallback also failed: {str(cpu_error)}\")\n            clear_gpu_memory()\n            return None, None\n\n# Create the main loading function\ndef load_model_with_config(model_name):\n    \"\"\"Main model loading function with safe error handling\"\"\"\n    return load_model_with_config_memory_optimized(model_name)\n\nprint(\"üß† Updated memory-optimized model loading function\")\nprint(\"üõ°Ô∏è Added safe error handling and CPU fallback\")\nprint(\"‚ö° Removed quantization to avoid bitsandbytes dependency\")\nprint(\"üîß FIXED: Added short model name mapping (deepseek-coder-6.7b ‚Üí deepseek-ai/deepseek-coder-6.7b-instruct)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:10:09.349774Z","iopub.execute_input":"2025-11-14T19:10:09.350531Z","iopub.status.idle":"2025-11-14T19:10:09.361302Z","shell.execute_reply.started":"2025-11-14T19:10:09.350503Z","shell.execute_reply":"2025-11-14T19:10:09.360670Z"}},"outputs":[{"name":"stdout","text":"üß† Updated memory-optimized model loading function\nüõ°Ô∏è Added safe error handling and CPU fallback\n‚ö° Removed quantization to avoid bitsandbytes dependency\nüîß FIXED: Added short model name mapping (deepseek-coder-6.7b ‚Üí deepseek-ai/deepseek-coder-6.7b-instruct)\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# üîß FIX: Better Error Handling for Failed Model Loading\n\n# First define the original batch_generate_responses function\ndef batch_generate_responses(model, tokenizer, samples, generation_config=None, batch_size=None):\n    \"\"\"Original batch generation function\"\"\"\n    import torch\n    \n    if batch_size is None:\n        batch_size = 1  # Process one at a time to be safe\n    \n    if generation_config is None:\n        generation_config = {\n            'max_new_tokens': 512,\n            'temperature': 0.7,\n            'do_sample': True,\n            'pad_token_id': tokenizer.eos_token_id\n        }\n    \n    responses = []\n    \n    for i in range(0, len(samples), batch_size):\n        batch_samples = samples[i:i+batch_size]\n        prompts = []\n        \n        for sample in batch_samples:\n            if isinstance(sample, dict):\n                context = sample.get('context', [])\n                if context:\n                    prompt_parts = []\n                    for ctx in context:\n                        if isinstance(ctx, dict):\n                            content = ctx.get('content', str(ctx))\n                        else:\n                            content = str(ctx)\n                        prompt_parts.append(content)\n                    prompt = \"\\\\n\".join(prompt_parts)\n                else:\n                    # Handle raw GitHub issues\n                    title = sample.get('title', '')\n                    body = sample.get('body', '')\n                    prompt = f\"Issue: {title}\\\\n\\\\nDescription: {body}\"\n            else:\n                prompt = str(sample)\n            \n            prompts.append(prompt)\n        \n        # Generate responses\n        try:\n            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n            if torch.cuda.is_available():\n                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    **generation_config\n                )\n            \n            # Decode responses\n            for j, output in enumerate(outputs):\n                response = tokenizer.decode(output, skip_special_tokens=True)\n                # Remove the input prompt from the response\n                input_text = prompts[j]\n                if response.startswith(input_text):\n                    response = response[len(input_text):].strip()\n                responses.append(response)\n                \n        except Exception as e:\n            print(f\"‚ùå Generation error: {str(e)}\")\n            for _ in batch_samples:\n                responses.append(f\"[Generation failed: {str(e)}]\")\n    \n    return responses\n\n# Now define the safe version\ndef batch_generate_responses_safe(model, tokenizer, samples, generation_config=None, batch_size=None):\n    \"\"\"\n    Safe version of batch_generate_responses that handles None model AND creates proper samples\n    \"\"\"\n    # Check if model failed to load\n    if model is None or tokenizer is None:\n        print(\"‚ùå Model or tokenizer is None - cannot generate responses\")\n        \n        # Create proper sample structure for evaluation\n        processed_samples = []\n        for sample in samples:\n            if isinstance(sample, dict):\n                # Create a proper sample structure with all required fields\n                processed_sample = {\n                    'issue_number': sample.get('number', 'unknown'),\n                    'context': sample.get('context', []),\n                    'ground_truth': sample.get('ground_truth', '[No maintainer response]'),\n                    'turn_number': 1,\n                    'total_conversation_turns': 1,\n                    'maintainer_role': 'NONE',\n                    'total_maintainer_turns': 0,\n                    'prediction': '[Model failed to load]',\n                    'failed_generation': True\n                }\n                processed_samples.append(processed_sample)\n            else:\n                # Simple fallback\n                processed_samples.append({\n                    'issue_number': 'unknown',\n                    'context': [],\n                    'ground_truth': '[No ground truth available]',\n                    'turn_number': 1,\n                    'total_conversation_turns': 1,\n                    'maintainer_role': 'NONE',\n                    'total_maintainer_turns': 0,\n                    'prediction': '[Model failed to load]',\n                    'failed_generation': True\n                })\n        \n        # Return both responses and processed samples (matching expected return format)\n        responses = [\"[Model failed to load]\" for _ in samples]\n        return responses\n    \n    # Call the original function\n    return batch_generate_responses_original(model, tokenizer, samples, generation_config, batch_size)\n\n# Also create a safe model loading wrapper\ndef load_model_safe(model_name):\n    \"\"\"\n    Safe model loading that handles memory issues gracefully\n    \"\"\"\n    try:\n        print(f\"üîÑ Attempting to load {model_name}...\")\n        model, tokenizer = load_model_with_config(model_name)\n        \n        if model is None or tokenizer is None:\n            print(\"‚ùå Model loading returned None - evaluation will continue with placeholder responses\")\n            return None, None\n        \n        return model, tokenizer\n        \n    except Exception as e:\n        print(f\"‚ùå Exception during model loading: {str(e)}\")\n        clear_gpu_memory()\n        return None, None\n\n# Create a function to ensure samples have required structure\ndef ensure_sample_structure(samples):\n    \"\"\"\n    Ensure all samples have the required structure for evaluation\n    \"\"\"\n    fixed_samples = []\n    for sample in samples:\n        if isinstance(sample, dict):\n            # Ensure all required keys exist\n            fixed_sample = {\n                'issue_number': sample.get('number', sample.get('issue_number', 'unknown')),\n                'context': sample.get('context', []),\n                'ground_truth': sample.get('ground_truth', sample.get('body', '[No ground truth]')),\n                'turn_number': sample.get('turn_number', 1),\n                'total_conversation_turns': sample.get('total_conversation_turns', 1),\n                'maintainer_role': sample.get('maintainer_role', 'NONE'),\n                'total_maintainer_turns': sample.get('total_maintainer_turns', 0)\n            }\n            \n            # If this is a raw GitHub issue, try to extract some meaningful ground truth\n            if 'title' in sample and 'body' in sample and not fixed_sample['ground_truth']:\n                fixed_sample['ground_truth'] = f\"Issue: {sample.get('title', '')} - {sample.get('body', '')[:200]}...\"\n                \n            fixed_samples.append(fixed_sample)\n        else:\n            # Fallback for non-dict samples\n            fixed_samples.append({\n                'issue_number': 'unknown',\n                'context': [],\n                'ground_truth': '[No ground truth available]',\n                'turn_number': 1,\n                'total_conversation_turns': 1,\n                'maintainer_role': 'NONE',\n                'total_maintainer_turns': 0\n            })\n    \n    return fixed_samples\n\n# Override the batch function to be safe\nbatch_generate_responses_original = batch_generate_responses\nbatch_generate_responses = batch_generate_responses_safe\n\nprint(\"üõ°Ô∏è Added safe error handling for model loading failures\")\nprint(\"‚úÖ batch_generate_responses now handles None models gracefully\")\nprint(\"üîß Added sample structure validation to prevent 'ground_truth' errors\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:10:13.881675Z","iopub.execute_input":"2025-11-14T19:10:13.882295Z","iopub.status.idle":"2025-11-14T19:10:13.899656Z","shell.execute_reply.started":"2025-11-14T19:10:13.882270Z","shell.execute_reply":"2025-11-14T19:10:13.898695Z"}},"outputs":[{"name":"stdout","text":"üõ°Ô∏è Added safe error handling for model loading failures\n‚úÖ batch_generate_responses now handles None models gracefully\nüîß Added sample structure validation to prevent 'ground_truth' errors\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"# üîß Required helper functions for safe evaluation\ndef format_conversation_context(context):\n    \"\"\"Safely format conversation context\"\"\"\n    if not context:\n        return \"\"\n    \n    formatted = []\n    for item in context:\n        if isinstance(item, dict):\n            content = item.get('content', str(item))\n        else:\n            content = str(item)\n        formatted.append(content)\n    \n    return \"\\\\n\".join(formatted)\n\ndef calculate_all_metrics(prediction, ground_truth):\n    \"\"\"Calculate all evaluation metrics safely\"\"\"\n    try:\n        from evaluate import load\n        import nltk\n        from nltk.translate.bleu_score import sentence_bleu\n        from rouge_score import rouge_scorer\n        import difflib\n        \n        # Ensure strings\n        pred_str = str(prediction) if prediction else \"\"\n        gt_str = str(ground_truth) if ground_truth else \"\"\n        \n        # BLEU Score\n        try:\n            # Tokenize for BLEU\n            pred_tokens = pred_str.split()\n            gt_tokens = [gt_str.split()]  # List of reference tokenizations\n            bleu_score = sentence_bleu(gt_tokens, pred_tokens) if pred_tokens and gt_tokens[0] else 0.0\n        except:\n            bleu_score = 0.0\n        \n        # ROUGE Scores\n        try:\n            scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n            rouge_scores = scorer.score(gt_str, pred_str)\n            rouge_1 = rouge_scores['rouge1'].fmeasure\n            rouge_l = rouge_scores['rougeL'].fmeasure\n        except:\n            rouge_1 = 0.0\n            rouge_l = 0.0\n        \n        # Edit Similarity (using SequenceMatcher)\n        try:\n            edit_sim = difflib.SequenceMatcher(None, pred_str, gt_str).ratio()\n        except:\n            edit_sim = 0.0\n        \n        return {\n            'bleu': bleu_score,\n            'rouge_l': rouge_l,\n            'rouge_1': rouge_1,\n            'edit_similarity': edit_sim\n        }\n        \n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è Metrics calculation error: {str(e)}\")\n        return {\n            'bleu': 0.0,\n            'rouge_l': 0.0,\n            'rouge_1': 0.0,\n            'edit_similarity': 0.0\n        }\n\nprint(\"‚úÖ Helper functions loaded: format_conversation_context, calculate_all_metrics\")\nprint(\"üîß All functions now handle errors gracefully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:21:22.512664Z","iopub.execute_input":"2025-11-14T19:21:22.513546Z","iopub.status.idle":"2025-11-14T19:21:22.522788Z","shell.execute_reply.started":"2025-11-14T19:21:22.513513Z","shell.execute_reply":"2025-11-14T19:21:22.521801Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Helper functions loaded: format_conversation_context, calculate_all_metrics\nüîß All functions now handle errors gracefully\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"# üîß FINAL FIX: Safe evaluation function that handles missing ground_truth\nimport pandas as pd\nimport numpy as np\nimport time\nfrom tqdm import tqdm\n\ndef evaluate_model_comprehensive_SAFE(model, tokenizer, samples, model_name, \n                                     max_samples=None, generation_config=None, \n                                     save_results=True):\n    \"\"\"\n    SAFE comprehensive model evaluation that handles missing ground_truth keys\n    \"\"\"\n    \n    # FIRST: Ensure all samples have proper structure\n    print(\"üîß Validating sample structure...\")\n    safe_samples = ensure_sample_structure(samples)\n    print(f\"‚úÖ Validated {len(safe_samples)} samples\")\n    \n    # Determine sample size\n    if max_samples and max_samples < len(safe_samples):\n        # Use stratified sampling to get representative samples\n        sample_indices = np.linspace(0, len(safe_samples)-1, max_samples, dtype=int)\n        eval_samples = [safe_samples[i] for i in sample_indices]\n        print(f\"üìä Evaluating on {max_samples:,} stratified samples (from {len(safe_samples):,} total)\")\n    else:\n        eval_samples = safe_samples\n        print(f\"üìä Evaluating on all {len(eval_samples):,} samples\")\n    \n    results = []\n    failed_generations = 0\n    start_time = time.time()\n    \n    print(f\"üöÄ Starting evaluation of {model_name}...\")\n    \n    # Generate responses with safe batch processing\n    responses = batch_generate_responses(\n        model, tokenizer, eval_samples, \n        batch_size=1, generation_config=generation_config\n    )\n    \n    # Calculate metrics for each response\n    print(\"üìè Calculating evaluation metrics...\")\n    for i, (sample, response) in enumerate(tqdm(zip(eval_samples, responses), \n                                                desc=\"Computing metrics\", \n                                                total=len(eval_samples))):\n        \n        try:\n            # SAFE: Ensure we have ground_truth\n            ground_truth = sample.get('ground_truth', '[No ground truth available]')\n            \n            if not response or response == \"[Model failed to load]\":  # Handle failed generations\n                failed_generations += 1\n                metrics = {'bleu': 0.0, 'rouge_l': 0.0, 'rouge_1': 0.0, 'edit_similarity': 0.0}\n            else:\n                metrics = calculate_all_metrics(response, ground_truth)\n            \n            # SAFE: Format conversation context safely\n            context = sample.get('context', [])\n            if context:\n                try:\n                    context_length = len(format_conversation_context(context))\n                except:\n                    context_length = len(str(context))\n            else:\n                context_length = 0\n            \n            # Store comprehensive results with safe access\n            result = {\n                'model_name': model_name,\n                'sample_idx': i,\n                'issue_number': sample.get('issue_number', 'unknown'),\n                'turn_number': sample.get('turn_number', 1),\n                'total_turns': sample.get('total_conversation_turns', 1),\n                'context_length': context_length,\n                'context_turns': len(context),\n                'ground_truth_length': len(ground_truth),\n                'response_length': len(response) if response else 0,\n                'maintainer_role': sample.get('maintainer_role', 'NONE'),\n                'prediction': response,\n                'ground_truth': ground_truth,\n                **metrics\n            }\n            \n            results.append(result)\n            \n        except Exception as e:\n            print(f\"   ‚ùå Error processing sample {i}: {str(e)}\")\n            # Add a safe fallback result\n            results.append({\n                'model_name': model_name,\n                'sample_idx': i,\n                'issue_number': 'unknown',\n                'turn_number': 1,\n                'total_turns': 1,\n                'context_length': 0,\n                'context_turns': 0,\n                'ground_truth_length': 0,\n                'response_length': 0,\n                'maintainer_role': 'NONE',\n                'prediction': '[Processing failed]',\n                'ground_truth': '[No ground truth]',\n                'bleu': 0.0,\n                'rouge_l': 0.0,\n                'rouge_1': 0.0,\n                'edit_similarity': 0.0\n            })\n    \n    # Create results DataFrame\n    results_df = pd.DataFrame(results)\n    \n    # Calculate summary statistics\n    eval_time = time.time() - start_time\n    avg_metrics = results_df[['bleu', 'rouge_l', 'rouge_1', 'edit_similarity']].mean()\n    \n    # Print evaluation summary\n    print(f\"\\n{'='*60}\")\n    print(f\"üéØ EVALUATION COMPLETE: {model_name}\")\n    print(f\"{'='*60}\")\n    print(f\"‚è±Ô∏è  Total evaluation time: {eval_time/60:.1f} minutes\")\n    print(f\"üìä Samples evaluated: {len(results_df):,}\")\n    print(f\"‚ùå Failed generations: {failed_generations}\")\n    print(f\"‚úÖ Success rate: {(len(results_df)-failed_generations)/len(results_df)*100:.1f}%\")\n    \n    print(f\"\\nüìà Average Scores:\")\n    for metric, score in avg_metrics.items():\n        print(f\"   {metric.upper():15}: {score:.4f}\")\n    \n    overall_avg = avg_metrics.mean()\n    print(f\"   {'OVERALL AVG':15}: {overall_avg:.4f}\")\n    \n    # Save results if requested\n    if save_results:\n        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"evaluation_results_{model_name.replace('/', '_')}_{timestamp}.csv\"\n        results_df.to_csv(filename, index=False)\n        print(f\"üíæ Results saved to: {filename}\")\n    \n    return results_df, avg_metrics\n\n# Override the original function\nevaluate_model_comprehensive = evaluate_model_comprehensive_SAFE\n\nprint(\"üîß FIXED: evaluate_model_comprehensive now handles missing 'ground_truth' keys safely\")\nprint(\"üõ°Ô∏è Added comprehensive error handling and sample structure validation\")\nprint(\"‚úÖ No more 'ground_truth' key errors!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:20:50.750587Z","iopub.execute_input":"2025-11-14T19:20:50.751217Z","iopub.status.idle":"2025-11-14T19:20:50.770546Z","shell.execute_reply.started":"2025-11-14T19:20:50.751185Z","shell.execute_reply":"2025-11-14T19:20:50.769656Z"}},"outputs":[{"name":"stdout","text":"üîß FIXED: evaluate_model_comprehensive now handles missing 'ground_truth' keys safely\nüõ°Ô∏è Added comprehensive error handling and sample structure validation\n‚úÖ No more 'ground_truth' key errors!\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"# Quick test with first 2 repositories and limited samples\nquick_results = run_complete_multi_repository_evaluation(\n    repositories=detected_repos,\n    models_to_test=['deepseek-coder-1.3b'],  # Just one model for testing\n    max_repos=2,                             # Test only first 2 repositories\n    samples_per_repo=10                      # Only 10 samples per repo\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T19:21:32.696235Z","iopub.execute_input":"2025-11-14T19:21:32.697002Z","iopub.status.idle":"2025-11-14T19:25:58.443456Z","shell.execute_reply.started":"2025-11-14T19:21:32.696975Z","shell.execute_reply":"2025-11-14T19:25:58.442372Z"}},"outputs":[{"name":"stdout","text":"üöÄ COMPLETE MULTI-REPOSITORY EVALUATION\n======================================================================\nüìä Repositories: 28\nü§ñ Models: 1\nüìù Samples per repo: 10\nüéØ Total evaluations: 280\n======================================================================\n‚ö° Quick test mode: Testing only first 2 repositories\n\nüîç REPOSITORY: AutoGPT\nüìÅ Files: 2229 JSON files\n--------------------------------------------------\n   üìÇ Loading from: /kaggle/input/coderepoqa/AutoGPT/cloudide/workspace/QA_data\n   üìÑ Processing 2229 JSON files\n   ‚úÖ Loaded 2229 total samples\n   üéØ Selected 10 samples (stratified sampling)\n‚úÖ Loaded 10 samples from AutoGPT\n\nü§ñ Model 1/2: deepseek-coder-1.3b\nüìä Repository: AutoGPT\nüîÑ Model mapping: deepseek-coder-1.3b ‚Üí deepseek-ai/deepseek-coder-1.3b-instruct\nüîÑ Loading tokenizer for deepseek-ai/deepseek-coder-1.3b-instruct...\nüîÑ Loading model deepseek-ai/deepseek-coder-1.3b-instruct with memory optimization...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Successfully loaded deepseek-coder-1.3b (deepseek-ai/deepseek-coder-1.3b-instruct)\nüìä Model device: cuda:0\nüîç GPU Memory - Allocated: 2.52GB, Reserved: 2.67GB\nüîß Validating sample structure...\n‚úÖ Validated 10 samples\nüìä Evaluating on all 10 samples\nüöÄ Starting evaluation of deepseek-coder-1.3b...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"üìè Calculating evaluation metrics...\n","output_type":"stream"},{"name":"stderr","text":"Computing metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 43.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüéØ EVALUATION COMPLETE: deepseek-coder-1.3b\n============================================================\n‚è±Ô∏è  Total evaluation time: 2.1 minutes\nüìä Samples evaluated: 10\n‚ùå Failed generations: 0\n‚úÖ Success rate: 100.0%\n\nüìà Average Scores:\n   BLEU           : 0.0000\n   ROUGE_L        : 0.0671\n   ROUGE_1        : 0.1262\n   EDIT_SIMILARITY: 0.0133\n   OVERALL AVG    : 0.0517\n   ‚úÖ BLEU: 0.000\n   ‚úÖ ROUGE-L: 0.067\n   ‚úÖ Edit Sim: 0.013\n\nüìà REPOSITORY SUMMARY: AutoGPT\n   üèÜ Best BLEU: deepseek-coder-1.3b (0.000)\n   üèÜ Best ROUGE-L: deepseek-coder-1.3b (0.067)\n   üìä Avg BLEU: 0.000\n   üìä Avg ROUGE-L: 0.067\n\nüîç REPOSITORY: Pillow\nüìÅ Files: 2976 JSON files\n--------------------------------------------------\n   üìÇ Loading from: /kaggle/input/coderepoqa/Pillow/cloudide/workspace/QA_data\n   üìÑ Processing 2976 JSON files\n   ‚úÖ Loaded 2976 total samples\n   üéØ Selected 10 samples (stratified sampling)\n‚úÖ Loaded 10 samples from Pillow\n\nü§ñ Model 2/2: deepseek-coder-1.3b\nüìä Repository: Pillow\nüîÑ Model mapping: deepseek-coder-1.3b ‚Üí deepseek-ai/deepseek-coder-1.3b-instruct\nüîÑ Loading tokenizer for deepseek-ai/deepseek-coder-1.3b-instruct...\nüîÑ Loading model deepseek-ai/deepseek-coder-1.3b-instruct with memory optimization...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Successfully loaded deepseek-coder-1.3b (deepseek-ai/deepseek-coder-1.3b-instruct)\nüìä Model device: cuda:0\nüîç GPU Memory - Allocated: 2.52GB, Reserved: 2.67GB\nüîß Validating sample structure...\n‚úÖ Validated 10 samples\nüìä Evaluating on all 10 samples\nüöÄ Starting evaluation of deepseek-coder-1.3b...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nSetting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"üìè Calculating evaluation metrics...\n","output_type":"stream"},{"name":"stderr","text":"Computing metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 45.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüéØ EVALUATION COMPLETE: deepseek-coder-1.3b\n============================================================\n‚è±Ô∏è  Total evaluation time: 2.1 minutes\nüìä Samples evaluated: 10\n‚ùå Failed generations: 0\n‚úÖ Success rate: 100.0%\n\nüìà Average Scores:\n   BLEU           : 0.0000\n   ROUGE_L        : 0.0745\n   ROUGE_1        : 0.1378\n   EDIT_SIMILARITY: 0.0261\n   OVERALL AVG    : 0.0596\n   ‚úÖ BLEU: 0.000\n   ‚úÖ ROUGE-L: 0.075\n   ‚úÖ Edit Sim: 0.026\n\nüìà REPOSITORY SUMMARY: Pillow\n   üèÜ Best BLEU: deepseek-coder-1.3b (0.000)\n   üèÜ Best ROUGE-L: deepseek-coder-1.3b (0.075)\n   üìä Avg BLEU: 0.000\n   üìä Avg ROUGE-L: 0.075\n\n======================================================================\nüéØ COMPLETE MULTI-REPOSITORY EVALUATION FINISHED\n======================================================================\n\nüíæ Results saved:\n   üìÑ Raw results: multi_repo_results_20251114_192558.json\n   üìä Summary: repository_summary_20251114_192558.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":69}]}